{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_random_forest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnR6m4Jr6oAwQLGFw7l5+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dominika26/classification_tutorials_public/blob/main/TensorFlow_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Las decyzyjny TensorFlow**"
      ],
      "metadata": {
        "id": "mfIk0gyVELYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eohngl_CDw2A",
        "outputId": "83cd2abf-2527-4a2a-d581-ee75714a80dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-0.2.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 3.0 MB/s \n",
            "\u001b[?25hCollecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.21.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (0.37.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.15.0)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.3.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (1.44.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (13.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (0.24.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tensorflow_decision_forests) (0.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tensorflow_decision_forests) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tensorflow_decision_forests) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2018.9)\n",
            "Installing collected packages: tf-estimator-nightly, wurlitzer, tensorflow-decision-forests\n",
            "Successfully installed tensorflow-decision-forests-0.2.4 tf-estimator-nightly-2.8.0.dev2021122109 wurlitzer-3.0.2\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_decision_forests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wurlitzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox2u-2izEkcd",
        "outputId": "71dcf7f7-5dab-40b1-bb92-5d92e8230c9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.7/dist-packages (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "try:\n",
        "  from wurlitzer import sys_pipes\n",
        "except:\n",
        "  from colabtools.googlelog import CaptureLog as sys_pipes\n",
        "\n",
        "from IPython.core.magic import register_line_magic\n",
        "from IPython.display import Javascript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QYE-MCZErP2",
        "outputId": "39c6dc34-8ce1-4643-9016-e24d5fdd9b8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TF Parameter Server distributed training not available (this is expected for the pre-build release).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@register_line_magic \n",
        "def set_cell_height(size):\n",
        "  display(\n",
        "      Javascript(\"google.colab.output.setIframeHeight(0, true, {maxHeight: \" +\n",
        "                 str(size) + \"})\"))\n",
        "  "
      ],
      "metadata": {
        "id": "lqxK8tS3E4B1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the version of tensorflow decision forests\n",
        "print(\"Found TensorFlow Forests v\" + tfdf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BuZjStwFNuP",
        "outputId": "09cf8bce-e7fa-42a8-80ab-c36ecfcbf2b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found TensorFlow Forests v0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trening losowego modelu lasu**\n"
      ],
      "metadata": {
        "id": "yI5GSAZpFgua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Załadowanie zestawu danych i konwersja do tf.Dataset"
      ],
      "metadata": {
        "id": "opk2pyEaFsqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv\n",
        "\n",
        "# Load a dataset into a Pandas Dataframe.\n",
        "dataset_df = pd.read_csv(\"/tmp/penguins.csv\")\n",
        "\n",
        "# Display the first 3 examples.\n",
        "dataset_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "3ziXSWHYFelB",
        "outputId": "5208d32a-23ed-468e-959a-13f5b755b700"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
              "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
              "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
              "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
              "\n",
              "   body_mass_g     sex  year  \n",
              "0       3750.0    male  2007  \n",
              "1       3800.0  female  2007  \n",
              "2       3250.0  female  2007  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07cfddeb-cd76-4de7-a88b-522da26dd3f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "      <th>island</th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.5</td>\n",
              "      <td>17.4</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>40.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>3250.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07cfddeb-cd76-4de7-a88b-522da26dd3f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07cfddeb-cd76-4de7-a88b-522da26dd3f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07cfddeb-cd76-4de7-a88b-522da26dd3f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mamy tutaj dane numeryczne, kategoryczne (island) i brakujące funkcje, TF-DF obsługuje wszystkie te typy funkcji natywnie więc nie ma potrzeby przeprowadzania normalizacji czy hot coding jka to było w przypadku adoption example.\n",
        "\n",
        "Etykieta species jest przechowywana jako string, należy ją przekształcić na liczby całkowite."
      ],
      "metadata": {
        "id": "NMuSeKkEF8s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = \"species\"\n",
        "\n",
        "classes = dataset_df[label].unique().tolist()\n",
        "print(f\"Label classes: {classes}\")\n",
        "dataset_df[label] = dataset_df[label].map(classes.index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qCWQLiaF24T",
        "outputId": "cbdee792-4f26-45b8-8f6c-9d861c0b38a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes: ['Adelie', 'Gentoo', 'Chinstrap']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podział danych na zbiór testowy i treningowy "
      ],
      "metadata": {
        "id": "oMDmknHXHGkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
        "print(\"{} examples in training, {} examples for testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1av8sylHBIV",
        "outputId": "74f97dd2-d0c9-4942-8be9-bc5446184d12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245 examples in training, 99 examples for testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konwersja pd.DataFrame do tf.data.Dataset\n",
        "\n",
        "Czyli z tego co rozumiem, staramy się przwidzieć gatunek "
      ],
      "metadata": {
        "id": "wSRmlDamHwhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7NaFinAHuy9",
        "outputId": "e23ef106-453a-4802-acc0-36a1ae9ef39a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_decision_forests/keras/core.py:2036: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  features_dataframe = dataframe.drop(label, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trenowanie modelu**\n"
      ],
      "metadata": {
        "id": "XWHKqjrBIMOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "#Specify the model \n",
        "model_1 = tfdf.keras.RandomForestModel()\n",
        "\n",
        "#Evaluation metrics\n",
        "model_1.compile(\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#Train the model \n",
        "with sys_pipes():\n",
        "  model_1.fit(x=train_ds)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2ncszmF2IFOA",
        "outputId": "3a33edc3-205d-44bd-a442-37d51e80a328"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpe7olmov3 as temporary training directory\n",
            "Starting reading the dataset\n",
            "1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:05.755265\n",
            "Training model\n",
            "Model trained in 0:00:00.062724\n",
            "Compiling model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO kernel.cc:1153] Loading model from path\n",
            "["
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 6s 6s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO abstract_model.cc:1063] Engine \"RandomForestGeneric\" built\n",
            "[INFO kernel.cc:1001] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7feb1f1170e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7feb1f1170e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7feb1f1170e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ocena modelu**"
      ],
      "metadata": {
        "id": "51ShfhXEKFG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value}4f\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUo7IEtvIqsi",
        "outputId": "23becbe7-cecd-4a76-ccbc-7a0065948dcd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 808ms/step - loss: 0.0000e+00 - accuracy: 0.9697\n",
            "\n",
            "loss: 0.04f\n",
            "accuracy: 0.96969699859619144f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Przygotuj model do obsługi TensorFlow**\n"
      ],
      "metadata": {
        "id": "p-4s-O0iLH0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save(\"/tmp/my_saved_model1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AD_jPfgKwQ0",
        "outputId": "cff94ab1-9a20-420f-9006-80e0e9faab1d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as call_get_leaves while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/my_saved_model1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/my_saved_model1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Wykreśl model***"
      ],
      "metadata": {
        "id": "MeKguK3ILYe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfdf.model_plotter.plot_model_in_colab(model_1, tree_idx=0, max_depth=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ZPujbZgHLVh7",
        "outputId": "e690ac03-214e-4308-fce4-4d88eac7a269"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
              "<div id=\"tree_plot_628fad1da0be4e5dbb23a2ef86e38efd\"></div>\n",
              "<script>\n",
              "/*\n",
              " * Copyright 2021 Google LLC.\n",
              " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              " * you may not use this file except in compliance with the License.\n",
              " * You may obtain a copy of the License at\n",
              " *\n",
              " *     https://www.apache.org/licenses/LICENSE-2.0\n",
              " *\n",
              " * Unless required by applicable law or agreed to in writing, software\n",
              " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              " * See the License for the specific language governing permissions and\n",
              " * limitations under the License.\n",
              " */\n",
              "\n",
              "/**\n",
              " *  Plotting of decision trees generated by TF-DF.\n",
              " *\n",
              " *  A tree is a recursive structure of node objects.\n",
              " *  A node contains one or more of the following components:\n",
              " *\n",
              " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
              " *      the value is only present for analysis i.e. it is not used for\n",
              " *      predictions.\n",
              " *\n",
              " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
              " *      defines a binary test to branch to the positive or negative child.\n",
              " *\n",
              " *    - An explanation: Generally a plot showing the relation between the label\n",
              " *      and the condition to give insights about the effect of the condition.\n",
              " *\n",
              " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
              " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
              " *      red). The second children is the positive one (drawn in green).\n",
              " *\n",
              " */\n",
              "\n",
              "/**\n",
              " * Plots a single decision tree into a DOM element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!tree} raw_tree Recursive tree structure.\n",
              " * @param {string} canvas_id Id of the output dom element.\n",
              " */\n",
              "function display_tree(options, raw_tree, canvas_id) {\n",
              "  console.log(options);\n",
              "\n",
              "  // Determine the node placement.\n",
              "  const tree_struct = d3.tree().nodeSize(\n",
              "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
              "\n",
              "  // Boundaries of the node placement.\n",
              "  let x_min = Infinity;\n",
              "  let x_max = -x_min;\n",
              "  let y_min = Infinity;\n",
              "  let y_max = -x_min;\n",
              "\n",
              "  tree_struct.each(d => {\n",
              "    if (d.x > x_max) x_max = d.x;\n",
              "    if (d.x < x_min) x_min = d.x;\n",
              "    if (d.y > y_max) y_max = d.y;\n",
              "    if (d.y < y_min) y_min = d.y;\n",
              "  });\n",
              "\n",
              "  // Size of the plot.\n",
              "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
              "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
              "      options.node_y_offset - options.node_y_size;\n",
              "\n",
              "  const plot = d3.select(canvas_id);\n",
              "\n",
              "  // Tool tip\n",
              "  options.tooltip = plot.append('div')\n",
              "                        .attr('width', 100)\n",
              "                        .attr('height', 100)\n",
              "                        .style('padding', '4px')\n",
              "                        .style('background', '#fff')\n",
              "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
              "                        .style('border', '1px solid black')\n",
              "                        .style('font-family', 'sans-serif')\n",
              "                        .style('font-size', options.font_size)\n",
              "                        .style('position', 'absolute')\n",
              "                        .style('z-index', '10')\n",
              "                        .attr('pointer-events', 'none')\n",
              "                        .style('display', 'none');\n",
              "\n",
              "  // Create canvas\n",
              "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
              "  const graph =\n",
              "      svg.style('overflow', 'visible')\n",
              "          .append('g')\n",
              "          .attr('font-family', 'sans-serif')\n",
              "          .attr('font-size', options.font_size)\n",
              "          .attr(\n",
              "              'transform',\n",
              "              () => `translate(${options.margin},${\n",
              "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
              "\n",
              "  // Plot bounding box.\n",
              "  if (options.show_plot_bounding_box) {\n",
              "    svg.append('rect')\n",
              "        .attr('width', width)\n",
              "        .attr('height', height)\n",
              "        .attr('fill', 'none')\n",
              "        .attr('stroke-width', 1.0)\n",
              "        .attr('stroke', 'black');\n",
              "  }\n",
              "\n",
              "  // Draw the edges.\n",
              "  display_edges(options, graph, tree_struct);\n",
              "\n",
              "  // Draw the nodes.\n",
              "  display_nodes(options, graph, tree_struct);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Draw the nodes of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_nodes(options, graph, tree_struct) {\n",
              "  const nodes = graph.append('g')\n",
              "                    .selectAll('g')\n",
              "                    .data(tree_struct.descendants())\n",
              "                    .join('g')\n",
              "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
              "\n",
              "  nodes.append('rect')\n",
              "      .attr('x', 0.5)\n",
              "      .attr('y', 0.5)\n",
              "      .attr('width', options.node_x_size)\n",
              "      .attr('height', options.node_y_size)\n",
              "      .attr('stroke', 'lightgrey')\n",
              "      .attr('stroke-width', 1)\n",
              "      .attr('fill', 'white')\n",
              "      .attr('y', -options.node_y_size / 2);\n",
              "\n",
              "  // Brackets on the right of condition nodes without children.\n",
              "  non_leaf_node_without_children =\n",
              "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
              "          .append('g')\n",
              "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#F00');\n",
              "\n",
              "  non_leaf_node_without_children.append('path')\n",
              "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.0)\n",
              "      .attr('stroke', '#0F0');\n",
              "\n",
              "  const node_content = nodes.append('g').attr(\n",
              "      'transform',\n",
              "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
              "\n",
              "  node_content.append(node => create_node_element(options, node));\n",
              "}\n",
              "\n",
              "/**\n",
              " * Creates the D3 content for a single node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!node} node Node to draw.\n",
              " * @return {!d3} D3 content.\n",
              " */\n",
              "function create_node_element(options, node) {\n",
              "  // Output accumulator.\n",
              "  let output = {\n",
              "    // Content to draw.\n",
              "    content: d3.create('svg:g'),\n",
              "    // Vertical offset to the next element to draw.\n",
              "    vertical_offset: 0\n",
              "  };\n",
              "\n",
              "  // Conditions.\n",
              "  if (node.data.condition != null) {\n",
              "    display_condition(options, node.data.condition, output);\n",
              "  }\n",
              "\n",
              "  // Values.\n",
              "  if (node.data.value != null) {\n",
              "    display_value(options, node.data.value, output);\n",
              "  }\n",
              "\n",
              "  // Explanations.\n",
              "  if (node.data.explanation != null) {\n",
              "    display_explanation(options, node.data.explanation, output);\n",
              "  }\n",
              "\n",
              "  return output.content.node();\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text(options, text, output) {\n",
              "  output.content.append('text')\n",
              "      .attr('x', options.node_padding)\n",
              "      .attr('y', output.vertical_offset)\n",
              "      .attr('alignment-baseline', 'hanging')\n",
              "      .text(text);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a single line of text inside of a node with a tooltip.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {string} text Text to display.\n",
              " * @param {string} tooltip Text in the Tooltip.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
              "  const item = output.content.append('text')\n",
              "                   .attr('x', options.node_padding)\n",
              "                   .attr('alignment-baseline', 'hanging')\n",
              "                   .text(text);\n",
              "\n",
              "  add_tooltip(options, item, () => tooltip);\n",
              "  output.vertical_offset += 10;\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a tooltip to a dom element.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!dom} target Dom element to equip with a tooltip.\n",
              " * @param {!func} get_content Generates the html content of the tooltip.\n",
              " */\n",
              "function add_tooltip(options, target, get_content) {\n",
              "  function show(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.html(get_content());\n",
              "  }\n",
              "\n",
              "  function hide(d) {\n",
              "    options.tooltip.style('display', 'none');\n",
              "  }\n",
              "\n",
              "  function move(d) {\n",
              "    options.tooltip.style('display', 'block');\n",
              "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
              "    options.tooltip.style('top', d.pageY + 'px');\n",
              "  }\n",
              "\n",
              "  target.on('mouseover', show);\n",
              "  target.on('mouseout', hide);\n",
              "  target.on('mousemove', move);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a condition inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!condition} condition Condition to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_condition(options, condition, output) {\n",
              "  threshold_format = d3.format('r');\n",
              "\n",
              "  if (condition.type === 'IS_MISSING') {\n",
              "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'IS_TRUE') {\n",
              "    display_node_text(options, `${condition.attribute} is true`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
              "    format = d3.format('r');\n",
              "    display_node_text(\n",
              "        options,\n",
              "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} in [...]`,\n",
              "        `${condition.attribute} in [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `${condition.attribute} intersect [...]`,\n",
              "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
              "    display_node_text_with_tooltip(\n",
              "        options, `Sparse oblique split...`,\n",
              "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
              "            threshold_format(condition.threshold)}`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported condition ${condition.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds a value inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!value} value Value to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_value(options, value, output) {\n",
              "  if (value.type === 'PROBABILITY') {\n",
              "    const left_margin = 0;\n",
              "    const right_margin = 50;\n",
              "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
              "        left_margin - right_margin;\n",
              "\n",
              "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
              "    cusum.unshift(0);\n",
              "    const distribution_plot = output.content.append('g').attr(\n",
              "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
              "\n",
              "    distribution_plot.selectAll('rect')\n",
              "        .data(value.distribution)\n",
              "        .join('rect')\n",
              "        .attr('height', 10)\n",
              "        .attr(\n",
              "            'x',\n",
              "            (d, i) =>\n",
              "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
              "        .attr('width', (d, i) => d * plot_width)\n",
              "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
              "\n",
              "    const num_examples =\n",
              "        output.content.append('g')\n",
              "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
              "            .append('text')\n",
              "            .attr('x', options.node_x_size - options.node_padding)\n",
              "            .attr('alignment-baseline', 'hanging')\n",
              "            .attr('text-anchor', 'end')\n",
              "            .text(`(${value.num_examples})`);\n",
              "\n",
              "    const distribution_details = d3.create('ul');\n",
              "    distribution_details.selectAll('li')\n",
              "        .data(value.distribution)\n",
              "        .join('li')\n",
              "        .append('span')\n",
              "        .text(\n",
              "            (d, i) =>\n",
              "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
              "\n",
              "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
              "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
              "\n",
              "    output.vertical_offset += 10;\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  if (value.type === 'REGRESSION') {\n",
              "    display_node_text(\n",
              "        options,\n",
              "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
              "            d3.format('.6')(value.num_examples) + `)`,\n",
              "        output);\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Adds an explanation inside of a node.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!explanation} explanation Explanation to display.\n",
              " * @param {!output} output Output display accumulator.\n",
              " */\n",
              "function display_explanation(options, explanation, output) {\n",
              "  // Margin before the explanation.\n",
              "  output.vertical_offset += 10;\n",
              "\n",
              "  display_node_text(\n",
              "      options, `Non supported explanation ${explanation.type}`, output);\n",
              "}\n",
              "\n",
              "\n",
              "/**\n",
              " * Draw the edges of the tree.\n",
              " * @param {!options} options Dictionary of configurations.\n",
              " * @param {!graph} graph D3 search handle containing the graph.\n",
              " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
              " *     data, etc.).\n",
              " */\n",
              "function display_edges(options, graph, tree_struct) {\n",
              "  // Draw an edge between a parent and a child node with a bezier.\n",
              "  function draw_single_edge(d) {\n",
              "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
              "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
              "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
              "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
              "  }\n",
              "\n",
              "  graph.append('g')\n",
              "      .attr('fill', 'none')\n",
              "      .attr('stroke-width', 1.2)\n",
              "      .selectAll('path')\n",
              "      .data(tree_struct.links())\n",
              "      .join('path')\n",
              "      .attr('d', draw_single_edge)\n",
              "      .attr(\n",
              "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
              "}\n",
              "\n",
              "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.4530612244897959, 0.3306122448979592, 0.2163265306122449], \"num_examples\": 245.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"bill_length_mm\", \"threshold\": 43.099998474121094}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.015037593984962405, 0.6015037593984962, 0.38345864661654133], \"num_examples\": 133.0}, \"condition\": {\"type\": \"CATEGORICAL_IS_IN\", \"attribute\": \"island\", \"mask\": [\"Biscoe\"]}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.024390243902439025, 0.975609756097561, 0.0], \"num_examples\": 82.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"flipper_length_mm\", \"threshold\": 202.03292846679688}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.0, 1.0, 0.0], \"num_examples\": 77.0}}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.4, 0.6, 0.0], \"num_examples\": 5.0}}]}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.0, 0.0, 1.0], \"num_examples\": 51.0}}]}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.9732142857142857, 0.008928571428571428, 0.017857142857142856], \"num_examples\": 112.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"bill_length_mm\", \"threshold\": 41.900001525878906}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.75, 0.08333333333333333, 0.16666666666666666], \"num_examples\": 12.0}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"bill_depth_mm\", \"threshold\": 17.950000762939453}, \"children\": [{\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [1.0, 0.0, 0.0], \"num_examples\": 7.0}}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [0.4, 0.2, 0.4], \"num_examples\": 5.0}}]}, {\"value\": {\"type\": \"PROBABILITY\", \"distribution\": [1.0, 0.0, 0.0], \"num_examples\": 100.0}}]}]}, \"#tree_plot_628fad1da0be4e5dbb23a2ef86e38efd\")\n",
              "</script>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Struktura modelu i znaczenie funkcji**\n",
        "- Typ: algorytm uczenia używany do szkolenia modelu ( Random Forest w naszym przypadku).\n",
        "- Zadanie: Problem rozwiązany przez model ( Classification w naszym przypadku).\n",
        "- Wejście Funkcje: Wejście cechy modelu.\n",
        "- Zmienna Znaczenie: Różne środki znaczenia poszczególnych funkcji dla modelu.\n",
        "- Out-of-bag oceny: Ocena out-of-bag modelu. Jest to tania i wydajna alternatywa dla walidacji krzyżowej.\n",
        "- Liczba drzew, węzły {} i innych danych: Dane statystyczne dotyczące struktury lasów decyzje."
      ],
      "metadata": {
        "id": "Mt8zDrnHL6CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_cell_height 300\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "1BYzfitlLq0O",
        "outputId": "4ef99307-f643-4fb7-e64c-1e45de0af19a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"random_forest_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            "=================================================================\n",
            "Total params: 1\n",
            "Trainable params: 0\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n",
            "Type: \"RANDOM_FOREST\"\n",
            "Task: CLASSIFICATION\n",
            "Label: \"__LABEL\"\n",
            "\n",
            "Input Features (7):\n",
            "\tbill_depth_mm\n",
            "\tbill_length_mm\n",
            "\tbody_mass_g\n",
            "\tflipper_length_mm\n",
            "\tisland\n",
            "\tsex\n",
            "\tyear\n",
            "\n",
            "No weights\n",
            "\n",
            "Variable Importance: MEAN_MIN_DEPTH:\n",
            "    1.           \"__LABEL\"  2.951803 ################\n",
            "    2.               \"sex\"  2.940447 ###############\n",
            "    3.              \"year\"  2.935885 ###############\n",
            "    4.       \"body_mass_g\"  2.452069 ###########\n",
            "    5.            \"island\"  2.183208 #########\n",
            "    6.     \"bill_depth_mm\"  2.163892 ########\n",
            "    7.    \"bill_length_mm\"  1.219276 \n",
            "    8. \"flipper_length_mm\"  1.170158 \n",
            "\n",
            "Variable Importance: NUM_AS_ROOT:\n",
            "    1. \"flipper_length_mm\" 153.000000 ################\n",
            "    2.    \"bill_length_mm\" 92.000000 #########\n",
            "    3.     \"bill_depth_mm\" 38.000000 ###\n",
            "    4.       \"body_mass_g\" 10.000000 \n",
            "    5.            \"island\"  7.000000 \n",
            "\n",
            "Variable Importance: NUM_NODES:\n",
            "    1.    \"bill_length_mm\" 537.000000 ################\n",
            "    2. \"flipper_length_mm\" 352.000000 ##########\n",
            "    3.     \"bill_depth_mm\" 325.000000 #########\n",
            "    4.            \"island\" 270.000000 #######\n",
            "    5.       \"body_mass_g\" 239.000000 ######\n",
            "    6.              \"year\" 14.000000 \n",
            "    7.               \"sex\" 10.000000 \n",
            "\n",
            "Variable Importance: SUM_SCORE:\n",
            "    1.    \"bill_length_mm\" 27431.664573 ################\n",
            "    2. \"flipper_length_mm\" 24267.038146 ##############\n",
            "    3.            \"island\" 10214.331825 #####\n",
            "    4.     \"bill_depth_mm\" 8780.057404 #####\n",
            "    5.       \"body_mass_g\" 3460.191693 #\n",
            "    6.               \"sex\" 89.810510 \n",
            "    7.              \"year\" 35.727843 \n",
            "\n",
            "\n",
            "\n",
            "Winner take all: true\n",
            "Out-of-bag evaluation: accuracy:0.963265 logloss:0.0806595\n",
            "Number of trees: 300\n",
            "Total number of nodes: 3794\n",
            "\n",
            "Number of nodes by tree:\n",
            "Count: 300 Average: 12.6467 StdDev: 2.92492\n",
            "Min: 7 Max: 27 Ignored: 0\n",
            "----------------------------------------------\n",
            "[  7,  8)  3   1.00%   1.00%\n",
            "[  8,  9)  0   0.00%   1.00%\n",
            "[  9, 10) 43  14.33%  15.33% ####\n",
            "[ 10, 11)  0   0.00%  15.33%\n",
            "[ 11, 12) 97  32.33%  47.67% ##########\n",
            "[ 12, 13)  0   0.00%  47.67%\n",
            "[ 13, 14) 80  26.67%  74.33% ########\n",
            "[ 14, 15)  0   0.00%  74.33%\n",
            "[ 15, 16) 40  13.33%  87.67% ####\n",
            "[ 16, 17)  0   0.00%  87.67%\n",
            "[ 17, 18) 24   8.00%  95.67% ##\n",
            "[ 18, 19)  0   0.00%  95.67%\n",
            "[ 19, 20)  6   2.00%  97.67% #\n",
            "[ 20, 21)  0   0.00%  97.67%\n",
            "[ 21, 22)  5   1.67%  99.33% #\n",
            "[ 22, 23)  0   0.00%  99.33%\n",
            "[ 23, 24)  0   0.00%  99.33%\n",
            "[ 24, 25)  0   0.00%  99.33%\n",
            "[ 25, 26)  1   0.33%  99.67%\n",
            "[ 26, 27]  1   0.33% 100.00%\n",
            "\n",
            "Depth by leafs:\n",
            "Count: 2047 Average: 3.03175 StdDev: 0.931702\n",
            "Min: 1 Max: 8 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 1, 2)  14   0.68%   0.68%\n",
            "[ 2, 3) 637  31.12%  31.80% ########\n",
            "[ 3, 4) 805  39.33%  71.13% ##########\n",
            "[ 4, 5) 482  23.55%  94.68% ######\n",
            "[ 5, 6)  86   4.20%  98.88% #\n",
            "[ 6, 7)  18   0.88%  99.76%\n",
            "[ 7, 8)   3   0.15%  99.90%\n",
            "[ 8, 8]   2   0.10% 100.00%\n",
            "\n",
            "Number of training obs by leaf:\n",
            "Count: 2047 Average: 35.9062 StdDev: 34.9421\n",
            "Min: 5 Max: 115 Ignored: 0\n",
            "----------------------------------------------\n",
            "[   5,  10) 927  45.29%  45.29% ##########\n",
            "[  10,  16)  97   4.74%  50.02% #\n",
            "[  16,  21)  37   1.81%  51.83%\n",
            "[  21,  27)  53   2.59%  54.42% #\n",
            "[  27,  32)  52   2.54%  56.96% #\n",
            "[  32,  38)  72   3.52%  60.48% #\n",
            "[  38,  43)  77   3.76%  64.24% #\n",
            "[  43,  49)  68   3.32%  67.56% #\n",
            "[  49,  54)  44   2.15%  69.71%\n",
            "[  54,  60)  32   1.56%  71.28%\n",
            "[  60,  66)  26   1.27%  72.55%\n",
            "[  66,  71)  30   1.47%  74.01%\n",
            "[  71,  77)  65   3.18%  77.19% #\n",
            "[  77,  82)  92   4.49%  81.68% #\n",
            "[  82,  88) 100   4.89%  86.57% #\n",
            "[  88,  93)  89   4.35%  90.91% #\n",
            "[  93,  99)  96   4.69%  95.60% #\n",
            "[  99, 104)  47   2.30%  97.90% #\n",
            "[ 104, 110)  32   1.56%  99.46%\n",
            "[ 110, 115]  11   0.54% 100.00%\n",
            "\n",
            "Attribute in nodes:\n",
            "\t537 : bill_length_mm [NUMERICAL]\n",
            "\t352 : flipper_length_mm [NUMERICAL]\n",
            "\t325 : bill_depth_mm [NUMERICAL]\n",
            "\t270 : island [CATEGORICAL]\n",
            "\t239 : body_mass_g [NUMERICAL]\n",
            "\t14 : year [NUMERICAL]\n",
            "\t10 : sex [CATEGORICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 0:\n",
            "\t153 : flipper_length_mm [NUMERICAL]\n",
            "\t92 : bill_length_mm [NUMERICAL]\n",
            "\t38 : bill_depth_mm [NUMERICAL]\n",
            "\t10 : body_mass_g [NUMERICAL]\n",
            "\t7 : island [CATEGORICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 1:\n",
            "\t257 : flipper_length_mm [NUMERICAL]\n",
            "\t256 : bill_length_mm [NUMERICAL]\n",
            "\t163 : bill_depth_mm [NUMERICAL]\n",
            "\t126 : island [CATEGORICAL]\n",
            "\t84 : body_mass_g [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 2:\n",
            "\t424 : bill_length_mm [NUMERICAL]\n",
            "\t309 : flipper_length_mm [NUMERICAL]\n",
            "\t267 : bill_depth_mm [NUMERICAL]\n",
            "\t224 : island [CATEGORICAL]\n",
            "\t188 : body_mass_g [NUMERICAL]\n",
            "\t5 : year [NUMERICAL]\n",
            "\t4 : sex [CATEGORICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 3:\n",
            "\t519 : bill_length_mm [NUMERICAL]\n",
            "\t341 : flipper_length_mm [NUMERICAL]\n",
            "\t316 : bill_depth_mm [NUMERICAL]\n",
            "\t262 : island [CATEGORICAL]\n",
            "\t227 : body_mass_g [NUMERICAL]\n",
            "\t12 : year [NUMERICAL]\n",
            "\t9 : sex [CATEGORICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 5:\n",
            "\t537 : bill_length_mm [NUMERICAL]\n",
            "\t351 : flipper_length_mm [NUMERICAL]\n",
            "\t324 : bill_depth_mm [NUMERICAL]\n",
            "\t270 : island [CATEGORICAL]\n",
            "\t238 : body_mass_g [NUMERICAL]\n",
            "\t14 : year [NUMERICAL]\n",
            "\t10 : sex [CATEGORICAL]\n",
            "\n",
            "Condition type in nodes:\n",
            "\t1467 : HigherCondition\n",
            "\t280 : ContainsBitmapCondition\n",
            "Condition type in nodes with depth <= 0:\n",
            "\t293 : HigherCondition\n",
            "\t7 : ContainsBitmapCondition\n",
            "Condition type in nodes with depth <= 1:\n",
            "\t760 : HigherCondition\n",
            "\t126 : ContainsBitmapCondition\n",
            "Condition type in nodes with depth <= 2:\n",
            "\t1193 : HigherCondition\n",
            "\t228 : ContainsBitmapCondition\n",
            "Condition type in nodes with depth <= 3:\n",
            "\t1415 : HigherCondition\n",
            "\t271 : ContainsBitmapCondition\n",
            "Condition type in nodes with depth <= 5:\n",
            "\t1464 : HigherCondition\n",
            "\t280 : ContainsBitmapCondition\n",
            "Node format: NOT_SET\n",
            "\n",
            "Training OOB:\n",
            "\ttrees: 1, Out-of-bag evaluation: accuracy:0.913043 logloss:3.13423\n",
            "\ttrees: 11, Out-of-bag evaluation: accuracy:0.963115 logloss:0.359223\n",
            "\ttrees: 21, Out-of-bag evaluation: accuracy:0.959184 logloss:0.229809\n",
            "\ttrees: 31, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0843826\n",
            "\ttrees: 41, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0857535\n",
            "\ttrees: 51, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0906413\n",
            "\ttrees: 61, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0846032\n",
            "\ttrees: 71, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0861476\n",
            "\ttrees: 81, Out-of-bag evaluation: accuracy:0.967347 logloss:0.087049\n",
            "\ttrees: 91, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0871696\n",
            "\ttrees: 101, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0814888\n",
            "\ttrees: 111, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0813054\n",
            "\ttrees: 121, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0825762\n",
            "\ttrees: 131, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0833503\n",
            "\ttrees: 141, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0846085\n",
            "\ttrees: 151, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0845112\n",
            "\ttrees: 161, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0843167\n",
            "\ttrees: 171, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0832224\n",
            "\ttrees: 181, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0828926\n",
            "\ttrees: 191, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0818861\n",
            "\ttrees: 201, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0814035\n",
            "\ttrees: 211, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0807423\n",
            "\ttrees: 221, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0810588\n",
            "\ttrees: 231, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0811493\n",
            "\ttrees: 241, Out-of-bag evaluation: accuracy:0.967347 logloss:0.0810817\n",
            "\ttrees: 251, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0811255\n",
            "\ttrees: 261, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0811532\n",
            "\ttrees: 271, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0806681\n",
            "\ttrees: 281, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0812389\n",
            "\ttrees: 291, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0803448\n",
            "\ttrees: 300, Out-of-bag evaluation: accuracy:0.963265 logloss:0.0806595\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.make_inspector().features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eCCuKAgMTSs",
        "outputId": "65b04219-b841-4fcb-ee2f-f1c692ce2926"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"bill_depth_mm\" (1; #0),\n",
              " \"bill_length_mm\" (1; #1),\n",
              " \"body_mass_g\" (1; #2),\n",
              " \"flipper_length_mm\" (1; #3),\n",
              " \"island\" (4; #4),\n",
              " \"sex\" (4; #5),\n",
              " \"year\" (1; #6)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.make_inspector().variable_importances()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGQZF38FMji3",
        "outputId": "1c8974f3-de4f-40ec-cb9d-da84b75e1a00"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MEAN_MIN_DEPTH': [(\"__LABEL\" (4; #7), 2.9518032893032857),\n",
              "  (\"sex\" (4; #5), 2.9404474691974656),\n",
              "  (\"year\" (1; #6), 2.9358852998852973),\n",
              "  (\"body_mass_g\" (1; #2), 2.4520688385688363),\n",
              "  (\"island\" (4; #4), 2.1832082269582296),\n",
              "  (\"bill_depth_mm\" (1; #0), 2.1638924963924984),\n",
              "  (\"bill_length_mm\" (1; #1), 1.2192762052762067),\n",
              "  (\"flipper_length_mm\" (1; #3), 1.170158175158175)],\n",
              " 'NUM_AS_ROOT': [(\"flipper_length_mm\" (1; #3), 153.0),\n",
              "  (\"bill_length_mm\" (1; #1), 92.0),\n",
              "  (\"bill_depth_mm\" (1; #0), 38.0),\n",
              "  (\"body_mass_g\" (1; #2), 10.0),\n",
              "  (\"island\" (4; #4), 7.0)],\n",
              " 'NUM_NODES': [(\"bill_length_mm\" (1; #1), 537.0),\n",
              "  (\"flipper_length_mm\" (1; #3), 352.0),\n",
              "  (\"bill_depth_mm\" (1; #0), 325.0),\n",
              "  (\"island\" (4; #4), 270.0),\n",
              "  (\"body_mass_g\" (1; #2), 239.0),\n",
              "  (\"year\" (1; #6), 14.0),\n",
              "  (\"sex\" (4; #5), 10.0)],\n",
              " 'SUM_SCORE': [(\"bill_length_mm\" (1; #1), 27431.66457260959),\n",
              "  (\"flipper_length_mm\" (1; #3), 24267.03814629838),\n",
              "  (\"island\" (4; #4), 10214.331824848428),\n",
              "  (\"bill_depth_mm\" (1; #0), 8780.057403685525),\n",
              "  (\"body_mass_g\" (1; #2), 3460.1916926661506),\n",
              "  (\"sex\" (4; #5), 89.81051023304462),\n",
              "  (\"year\" (1; #6), 35.72784338798374)]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Samoocena modelu**"
      ],
      "metadata": {
        "id": "BehczKYiM1EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.make_inspector().evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNshXqXZM0gC",
        "outputId": "e8342807-bcae-4ce2-d406-bcfedf5f168b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08065945486899237, rmse=None, ndcg=None, aucs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tworzenie logów treningowych**\n",
        "\n",
        "Pokazują jakość modelu zgodnie z liczbą drzew w modelu. "
      ],
      "metadata": {
        "id": "gvIq6BJxNDuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_cell_height 150\n",
        "model_1.make_inspector().training_logs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "y6WQV3TqMsDl",
        "outputId": "9810115c-c8cb-495f-ab77-839ea57d1ac0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 150})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TrainLog(num_trees=1, evaluation=Evaluation(num_examples=92, accuracy=0.9130434782608695, loss=3.1342305722443955, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=11, evaluation=Evaluation(num_examples=244, accuracy=0.9631147540983607, loss=0.359223054630346, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=21, evaluation=Evaluation(num_examples=245, accuracy=0.9591836734693877, loss=0.22980911111345095, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=31, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08438260880660038, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=41, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08575351507383949, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=51, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.09064132872284675, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=61, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08460320244942393, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=71, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08614760602311212, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=81, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08704904717754344, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=91, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08716958902624189, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=101, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08148875257798603, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=111, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08130538319902761, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=121, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08257615829304774, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=131, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08335032438745305, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=141, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08460853834997635, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=151, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08451118122178072, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=161, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08431672251650266, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=171, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08322237956584716, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=181, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08289262756173099, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=191, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08188608527487638, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=201, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.081403472468409, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=211, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08074233095560755, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=221, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08105878799545521, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=231, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08114931732036021, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=241, evaluation=Evaluation(num_examples=245, accuracy=0.9673469387755103, loss=0.08108174030726054, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=251, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08112554433850609, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=261, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08115315094438134, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=271, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08066805743669368, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=281, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08123891078771986, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=291, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.0803447759995351, rmse=None, ndcg=None, aucs=None)),\n",
              " TrainLog(num_trees=300, evaluation=Evaluation(num_examples=245, accuracy=0.963265306122449, loss=0.08065945486899237, rmse=None, ndcg=None, aucs=None))]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykres"
      ],
      "metadata": {
        "id": "1YSSGl7iNbuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "logs = model_1.make_inspector().training_logs()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Logloss (out-of-bag)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "x3jhnZwrNZAe",
        "outputId": "97f47ddf-bb58-4960-9885-2a90346d408b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAEKCAYAAADZxnkxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxcdb3/8dc7mSw0aQvSsnWh7FB2LFVQ2QQuuICAXsEV5IpXxevG/QmIqLigF+XKdUdFwAVUZCmIApddvULL2o2lIEsLXaAsTUqTTvL5/XHOpEPIMm0ycyaT9/PBPHL2+WQSTj/5ns/3+1VEYGZmZmZmQ1eXdQBmZmZmZrXCybWZmZmZ2TBxcm1mZmZmNkycXJuZmZmZDRMn12ZmZmZmw8TJtZmZmZnZMClrci3pCEkPS1ok6fQ+9m8t6WZJD0q6TdLkdPvBku4veq2R9K5yxmpmZmZmNlQq1zjXkuqBR4DDgMXAbOCEiFhQdMwfgOsi4hJJhwAnRcQHe13ndcAiYHJErC5LsGZmZmZmw6CcLdczgUUR8XhEdAKXA0f3OmY6cEu6fGsf+wHeDfzZibWZmZmZVbtcGa89CXi6aH0x8IZexzwAHAtcABwDjJW0aUQ8X3TM8cD5g73ZhAkTYtq0aUMK2MwsK/fcc89zETEx6zgqyfdtMxupBrpnlzO5LsVpwA8knQjcASwBugo7JW0J7A7c0NfJkk4BTgGYOnUqc+bMKXe8ZmZlIenJrGOotGnTpvm+bWYj0kD37HKWhSwBphStT0639YiIZyLi2IjYG/hiuu3FokP+FbgqItb29QYRcWFEzIiIGRMnjqoGHzMzMzOrQuVMrmcDO0jaRlIjSXnHrOIDJE2QVIjhDOCiXtc4AbisjDGamZmZmQ2bsiXXEZEHTiUp6VgI/D4i5ks6R9JR6WEHAQ9LegTYHPhG4XxJ00havm8vV4xmZmZmZsOprDXXEXE9cH2vbWcXLV8BXNHPuU+QdIo0MzMzMxsRPEOjmZmZmdkwcXJtZmZmZjZMnFybmZmZmQ2TrMe5HlUeePpF2jryvGn7CRV5vxvnL2XnLcYxddMxQ7rOc20d/G3Rcxy151ZIGnJM85a8NKRrjHQbNeZ43xumMn6jhqxDycRLq9dy80PLOHqvSdTXDe33yUaXxS+s5vezn+bdr58y5PuamVm5OLmuoC/88UGeeL6dmz57IFNeV95/GO56/HlO+dU97LBZK3/6j7fQmNuwhxQRwWcuv5+/LnqO7giO2XvyBsc054mVnPKrewAYYo4+okXAQ0tf5oLj9846lEyccdWDXD93KavW5Pnw/tOyDsdGkOWrOvifWxax99abOLk2s6rl5LpCnnp+NQ8tXQXAWVfP4+KT9h1yK3B/OvJdnHnVXMY153h0eRs/u/NxPnnw9ht0rVkPPMNfFz3HuOYcX79uIQfvtBkbj2lc7+us7ermi1fNY6vxzdz0uQNpaRq9v3r/fdMjXHDzo7z79ZN5yw6ja/KjWx5axvVzlzKuOcd5NzzMEbttwebjmrMOy0aI1vS+0d6RzzgSM7P+uea6Qm5csBSAk940jdsfWcGf5j5btve68PbHeWxFOxecsDdH7rYF/3Pzozz5fPt6X+el1Wv52nUL2HPKxvz2o2/kxVfW8q0/P7RBMf3szsd5eNkqzjl6t1GdWAN8/KDt2HZCC2ddPY81a7uyDqdiVnfm+dLV89l+s1au/MT+rO3q5qvXzs86LBtBWpxcm9kI4OS6Qm5csIydtxjLWW+fzm6TxvHVaxfw8po+Z3Ufkieea+f7ty7i7btvycE7bcaX37krDfV1fOma+UTEel3rW395iBdWr+Wbx+zGbpPGc/Kbt+Hy2U8z+4mV63Wdp55fzf/c/Cj/suvmHDp98/U6txY1N9Tz9WN248nnV/PDWxdlHU7FXHDzoyx58RW+eczubL/ZWD51yPZcP3cptzy0LOvQbIRobUyS67aO0fNHqZmNPE6uK+D5tg7mPLGSw6dvTn2dOPeYPXi+rYPz/vLwsL5PRHDW1fNoqq/j7HdOB2CL8c2cdviO3PHICq59sPTW8jlPrOSyu5/iI2+axq5bjQfgM4fuwKSNN+LMK+fSme8uOaYvXTOPeomvHLXr+n9TNWr/7SZw7N6T+Mntj7Fo+aqswym7h5a+zC/u/CfvnTGFmdu8DoBTDtiO7Tdr5UtXz2d1p1sibXAtTfWAW67NrLo5ua6Amx9aTnfA4btuAcDuk8fzof2m8eu7nuS+p14Ytvcp1Ef/5xE7vaqO9YP7TWOPyeM559oFvPTK4K3lxfXRnzl0x57tYxpznHP0rj113KX409xnuf2RFXz+8J3YcvxG6/9N1bAvvn0XWppynHnlPLq71++pwkjS3R2cceVcxm3UwOlH7tyzvTFXxzeP2Z0lL77CBTc/mmGENlLk6utobqijzcm1mVUxJ9cVcNOCZWw1vpldtxrXs+3zh+/I5mObOfOqeeS7SmsFHkhxffT737D1q/bV14lvHrM7K9s7OO+GwWumf37nP/utj37rLpv31HE/9fzqAa/z8pq1fPXaBew+abxHhejDpq1NnHHkztz9xEquuGdx1uGUzWWzn+K+p17krLfvwiYtr+4MO3Ob1/HeGVP4+Z3/ZOGzL2cUoQ1GUrOkuyU9IGm+pK/2cUyTpN9JWiTpLknTyhFLa1POybWZVTUn12X2SmcXdz66gsOmb/6q0UHGNjfwlaOms/DZl/nl354Y8vsU10f3NXbwbpPGc+L+2/Cbu57i3gFay59euZoLbn5kwProQh33WdfMG7CO+7y/PMzzbR1885jdPZ5xP97z+inMnPY6vvnnhTzf1pF1OMNu+ao1fOvPD7H/dptyzN6T+jzm9CN3ZvxGDZx51dyabsEf4TqAQyJiT2Av4AhJb+x1zMnACxGxPfDfwLfLEUhLU85lIWZW1Zxcl9kdj65gzdrunpKQYv+y6xYcustmnH/TIyx+YeBW4IEU6qNP2n9dfXRfPnf4jmwxrpkzr5zL2j5ayws124PVR28xvpnPp3Xc1/VTx33fUy/w67ue5MP7T2P3yf3HNNrV1YlvHLMb7R15vnH9wqzDGXZfv24hHWu7+dq7dut36MlNWhr54tt24b6nXuS3dz9V4QitFJFoS1cb0lfvv4SOBi5Jl68A3qoyjDfa0ujk2syqm5PrMrtpwTLGNed6OnEVU1ES++UNGM0DXl0f/dnDdhzw2NamHF85alceWrqKi/76z9fsX5/66A/tN43dJ43nq33Ucee7ujnzqnlsPraZzx++03p/T6PNDpuP5ZQDtuXKe5fw98eeyzqcYXPHIyuY9cAzfOLg7dhuYuuAxx67zyT223ZTvv2Xh1i+ak2FIrT1Iale0v3AcuCmiLir1yGTgKcBIiIPvARs2sd1TpE0R9KcFStWrHccrc0uCzGz6ubkuozyXd3cvHAZb91lcxrq+/6oJ28yhs8dtiM3P7ScG+YvXe/3KIwf/dUSx49OWss353v/+yhPr1zXWl6oj95t0riS6qPr68S5x/Zdx/3Lvz3Bwmdf5itHTe+Z9MEG9qlDdmDq68Zw1lXz6MiP/GHG1qzt4qyr57HthBY+ftB2gx4via8fs1vSyn1d7bXg14KI6IqIvYDJwExJu23gdS6MiBkRMWPixPWfRMk112ZW7Zxcl9GcJ1/ghdVrOWyQsZ1PetM0dtlyHF+ZtWC9/tF4euW68aMHe49iXz16VyT48qx1reWF+uhzj9mj5Provuq4F7+wmvNveoRDd9mMf+mjFMb61txQz9fftRuPP9fOj297LOtwhuwHtyziqZWr+foxu9GUqy/pnO0mtvKJg7fj2gee4Y5H1r9F0yojIl4EbgWO6LVrCTAFQFIOGA88P9zvn9Rcj/w/QM2sdjm5LqMb5y+jMVfHATsO3DqTq6/jm8fsxrJVa/jujaWNfV1qfXRfJm28EZ87bEdueWg5f5m3tKc++kP7rX99dO867i9fk8y495Wjdi3b9O616oAdJ3LUnlvxo1sf4/EVbYOfUKUeXbaKn97xGMfuM4n9t5uwXueO1tkrq52kiZI2Tpc3Ag4Deg89NAv4cLr8buCW2JBat0G0NtW75drMqpqT6zKJCG5auJQ3bz+hpNKIvaduwgfesDWX/P0J5i5+adDjhzp+9In7T2P6luP4yrXzOePKuWl99MA1230pruP+6KVzuPmh5XzusB2ZvMmY9b6WwVnv2IWmhjrOunrgkViqVXd38MWr5tHSlOOLb9tlvc9vyiWzVz61cjXfv8VjX1eRLYFbJT0IzCapub5O0jmSjkqP+QWwqaRFwOeA08sRiDs0mlm1c0FsCTrz3TTm1u/vkIeWruLpla/wyYO2L/mc/zxiJ/4yfymf/O297DVl4wGP/dui50quj+5Lrr6Obx67O8f86G8se7mDn3xgH8Y2N2zQtQp13P+7cBm7bDmOk960YTEZbDa2mS8csTNnXT2Pj1w8e4N/Jll56ZW13P3ESv7ruD3YtLVpg66x/3YTOHafSfz09sd5auUrjLTnHzttMZZPHlz6//cjQUQ8COzdx/azi5bXAO8pdywtTTlWd3bR1R0e4tPMqpKT60E89fxqDj3/di44fi+O3H3Lks+7cf4ypGTSlVKNa27gO+/Zk2/8aQFzlwzcer3Vxhvx7eNKr4/uy15TNub0I3Zm2csdQ66PPufoXcl3d3Pa4TuR66fzppXmfTOnct9TLw44Hnk1e++MKbxnxuQhXeOLb9uFZ19cw7xB/j+oRv11XrbhMbY5+WervTPPuBH2x6eZjQ5Orgfx9Aur6ezq5kvXzOON2276mhnm+nPTwqXsM3UTJo5dv9a7A3ecyIE7HrghoW6Qjx04+EgOpdhq4424+KSZw3Kt0a6uTnz3X/fMOoxMbdraxGWn9J6jxIyeUZHaO5xcm1l1chPLIAodZ55r6+Rrf1pQ0jlLXnyFeUte5vD1GMHDzMwGV5xcm5lVIyfXgyjcwN++x5Zcee8Sbi9hiLCb0vGq+5qV0czMNlxrUzK0Y5uH4zOzKuXkehCF5Pr0I3Zmu4ktnHnl3EFbTG5csIztN2tlmwktlQjRzGzUaGl0y7WZVTcn14MotI68rqWRbx+3B8+89Arn3dD/WNQvrV7LXf9c6ZIQM7MyaE07NHqsazOrVk6uB9HekUeCMY31zJj2Oj70xq255P+e4J4nV/Z5/C0PL6OrO1wSYmZWBoV5A9rWOLk2s+rk5HoQbR15WhpzPbMN/ucRO7PV+I34wh/n0pF/bc3fjfOXsfm4JvaYtH4zHZqZ2eB6OjR2Ork2s+rk5HoQ7R15WtIONJC0mnzjmN1YtLyNH96y6FXHrlnbxe2PrODQXTanzpMbmJkNu56Wa5eFmFmVKmtyLekISQ9LWiTpNVPhStpa0s2SHpR0m6TJRfumSrpR0kJJCyRNK2es/WnvzPe0lBQctNNmHLv3JH5022MsfPblnu1/f+w5Vnd2uSTEzKxMmnJ11NfJHRrNrGqVLbmWVA/8EDgSmA6cIGl6r8O+A1waEXsA5wDnFu27FDgvInYBZgLLyxXrQNo6unpaSop96R3TGb9RA1/444Pku7qBpCRkbFOO/bbdtNJhmpmNCpJobcrR7qH4zKxKlbPleiawKCIej4hO4HLg6F7HTAduSZdvLexPk/BcRNwEEBFtEbG6jLH2qz2tue5tk5ZGvnLUrjy4+CV++bcn6OoO/nfhMg7aeTMac662MTMrl9amHKvcodHMqlQ5s8BJwNNF64vTbcUeAI5Nl48BxkraFNgReFHSlZLuk3Re2hL+KpJOkTRH0pwVKwaf3GVDJDXXfc8S/449tuTQXTbnuzc9zDX3L+G5tk4O8xB8ZmZl1dJU77IQM6taWTexngYcKOk+4EBgCdAF5IC3pPv3BbYFTux9ckRcGBEzImLGxIkTyxJgW0e+Z0aw3iTx9XftRkNdHf/vigdpqBcH7VSeOMzMLNHSlPNoIWZWtcqZXC8BphStT0639YiIZyLi2IjYG/hiuu1Fklbu+9OSkjxwNbBPGWPt10At1wBbjG/mjLftQr472G+7CYxrbqhgdGZmo09rU86jhZhZ1eo/axy62cAOkrYhSaqPB95XfICkCcDKiOgGzgAuKjp3Y0kTI2IFcAgwp4yx9qu9nw6NxY7fdwpPrmzn4J02q1BUZmajV2tTjmUvr8k6DDOzPpUtuY6IvKRTgRuAeuCiiJgv6RxgTkTMAg4CzpUUwB3AJ9NzuySdBtysZPaWe4CflSvW/nTmu+ns6h6w5Rqgrk6cceQuFYrKzGx0a2nKeYZGM6ta5Wy5JiKuB67vte3souUrgCv6OfcmYI9yxjeYQoeZwZJrMzOrHJeFmFk1y7pDY1Ur3Lz769BoZmaV19JUT3tnFxGRdShmZq/h5HoAhd7obrk2M6seLU05urqDjnx31qGYmb2Gk+sBuCzEzKz6FDqZuzTEzKqRk+sBtKXT6w42WoiZmVVOT3LtTo1mVoWcXA+gp+W6j+nPzcwsGy1uuTazKubkegDrOjQ6uTaz0UnSFEm3Slogab6kT/dxzEGSXpJ0f/o6u69rDZfCPdlToJtZNXLWOIB1NdceLcTMRq088PmIuFfSWOAeSTdFxIJex90ZEe+oRECFlmtPgW5m1cgt1wNY3ZnUXLtDo5mNVhHxbETcmy6vAhYCk7KMqTA8aqFfjJlZNXFyPYC2jjy5OtGU88dkZiZpGrA3cFcfu/eT9ICkP0vadYBrnCJpjqQ5K1as2KA4WpsaAJeFmFl1ctY4gPaOPC1NOZIZ2M3MRi9JrcAfgc9ExMu9dt8LbB0RewLfB67u7zoRcWFEzIiIGRMnTtygWAqleh4txMyqkZPrAbR15N2Z0cxGPUkNJIn1byLiyt77I+LliGhLl68HGiRNKFc8hRGcPFqImVUjJ9cDSFqu3ZnRzEYvJY/ufgEsjIjz+zlmi/Q4JM0k+bfl+XLFVFcnxjTWuyzEzKqSm2UH0N7R5c6MZjbavQn4IDBX0v3ptjOBqQAR8RPg3cDHJeWBV4DjIyLKGVRLU86jhZhZVXLmOIC2jjxjm/0RmdnoFRF/BQbseBIRPwB+UJmIEmObch4txMyq0qCZo6Q6YE9gK5IWiXkRsbzcgVWD9o48W4xrzjoMMzPrpaUpR9uatVmHYWb2Gv0m15K2A74AHAo8CqwAmoEdJa0GfgpcEhHdlQg0C4XRQszMrLq0NNXT7pZrM6tCA2WOXwd+DHysd+2cpM2A95HU4V1SvvCylYwW4g6NZmbVprUpxzMvrsk6DDOz1+g3uY6IEwbYtxz4XlkiqhIRQXunOzSamVUjd2g0s2pVSs31sX1sfgmYW8u11x35brq6w8m1mdUESc3AO4C3UNSHBvhTRMzPMrYN0dKU81B8ZlaVSskcTwb2A25N1w8C7gG2kXRORPyqTLFlqjA5gSeRMbORTtJXSRLr20imLl9O2ocG+FaaeH8+Ih7MLMj1NLYpxyrP0GhmVaiUzDEH7BIRywAkbQ5cCrwBuAOoyeS60CLilmszqwF3R8SX+9l3ftqPZmolAxqqlqYcHflu8l3d5Oo9H5qZVY9SMscphcQ6tTzdtlJSzY6DtK7l2h0azWxki4g/DbJ/Ocm9fcQoNHy0d3QxfoyTazOrHqUk17dJug74Q7p+XLqtBXixbJFlrDDEk1uuzaxWSLoW6D1z4kvAHOCnETFiht8oNHy0deYZP6Yh42jMzNYpJXP8JElC/aZ0/VLgj+nwfAeXK7CsuSzEzGrQ48BE4LJ0/b3AKpLa65+RDK86IqxruXbdtZlVl0EzxzSJviJ9jRru0GhmNWj/iNi3aP1aSbMjYl9JI2rEkMK92Z0azazaDFqoJumNkmZLapPUKalL0suVCC5Lbrk2sxrUKqmn42K63JqudmYT0oZpdcu1mVWpUjLHHwDHk9RczwA+RPIIsab1tFw3Ork2s5rxeeCvkh4DBGwDfCLtQzOiZtt1WYiZVauSMseIWCSpPiK6gF9Kug84Y7DzJB0BXADUAz+PiG/12r81cBFJDeBK4AMRsTjd1wXMTQ99KiKOKvF7GhbrOjR6tBAzqw0Rcb2kHYCd000PF3ViHFGz7hZartucXJtZlSkluV4tqRG4X9J/Ac9SWjlJPfBD4DBgMTBb0qyIWFB02HeASyPiEkmHAOeyrkPNKxGx13p8L8OqvTNPU67O46eaWa3ZAdiJZBKZPSUREZdmHNN6c8u1mVWrUjLHD6bHnQq0A1NIRg8ZzExgUUQ8HhGdwOXA0b2OmQ7cki7f2sf+zLR15N2Z0cxqiqQvA99PXwcD/wVU9KngcCk8VWzv7Mo4EjOzVxs0uY6IJ4FuYBpwJXB6RCwq4dqTgKeL1hen24o9ABybLh8DjJW0abreLGmOpH9IeldfbyDplPSYOStWrCghpNK1d+TdmdHMas27gbcCSyPiJGBPYHy2IW2Yplw9jfV1Hi3EzKpOKeUdbwceA/6HpHPjIklHDtP7nwYcmNZwHwgsAQrNEFtHxAzgfcD3JG3X++SIuDAiZkTEjIkTJw5TSAkn12ZWg16JiG4gL2kc6Yy7Gce0wVqa6l0WYmZVp5Ts8bvAwYXW6jTJ/RPw50HOW8Krb9qT0209IuIZ0pZrSa3AcRHxYrpvSfr1cUm3AXuTJPkVkZSFuDOjmdWUOZI2Jpkw5h6gDfi/bEPacC1NOSfXZlZ1Sqm5XtWrDORxkhm9BjMb2EHSNmmHyOOBWcUHSJogqRDDGSQjhyBpE0lNhWNIZocs7ghZdu0dXW65NrOaEhGfiIgXI+InJJ3NP5yWh4xIrU05jxZiZlWn3+xRUqEWeo6k64HfAwG8hyRxHlBE5CWdCtxAMhTfRRExX9I5wJyImAUcBJwrKYA7SKZaB9gF+KmkbpI/AL7Va5SRsmvvyDN10zGVfEszs7JL7+1vJrmf/xV4MNuINlxrU472TifXZlZdBmqafWfR8jKSmmiAFSRDOA0qIq4Hru+17eyi5T6nVY+IvwO7l/Ie5dLWkfcEMmZWUyT9CNgeuCzd9DFJh0bEJwc4rWq1NOV4cfWImljSzEaBfrPHkfyocDi4Q6OZ1aBDgF0iIgAkXQLMzzakDdfalGPxC6uzDsPM7FXWa4YUSfeWK5Bq0t0dtHd2uUOjmdWaRcDUovUp6bYBSZoi6VZJCyTNl/TpPo6RpP+RtEjSg5L2Gca4+5SMFuJxrs2suqxv06zKEkWVWb22MPW5W67NbOSTdC1JjfVYYKGku9NdM4G7+z1xnTzw+Yi4V9JY4B5JN/XqC3MkyeyPOwBvAH6cfi0bjxZiZtVooA6Nn46ICyS9KSL+lm7+U4XiylThZu3k2sxqxHeGcnJEPAs8my6vkrSQZFKw4uT6aODStOTkH5I2lrRlem5ZtDblaOvMExFIo6Ltx8xGgIGyx5OAC0imyd0HICLOqkRQWSsM7eTpz82sFkTE7b23SXpHRFy3vteSNI1k3oG7eu3qb1besibXEbC600Onmln1GOhutFDSo8BWkoqHahIQEbFHeUPLjluuzWwUOAdYr+Q6nezrj8BnIuLlDXlTSacApwBMnTp1kKMHVrhHuwO6mVWTgUYLOUHSFiTjVB9VuZCy19aTXLtDo5nVrPWqo5DUQJJY/yYiruzjkEFn5QWIiAuBCwFmzJgR6xNDb4Wni20deTYbyoXMzIbRgKOFRMTSiNiT5LHe2PT1TEQ8WYngslLofe6yEDOrBZJuTr9+u2jzx9bjfAG/ABZGxPn9HDYL+FA6asgbgZfKWW8NxS3XHjHEzKrHoNmjpAOBS4EnSFo6pkj6cETcUebYMuOyEDOrMVtK2h84StLlJPfyfGG4vIgYbJjVNwEfBOZKuj/ddibpsH7pdOrXA28jGdpvNUm/nbIqPF30FOhmVk1KyR7PBw6PiIcBJO1IMrvX68sZWJbcodHMaszZwJdISjV6tzwHyeQy/YqIvzJIGUk6SkhFZ3oc29QA4OH4zKyqlJI9NhQSa4CIeCStvatZbrk2s1oSEVcAV0j6UkR8Let4hotbrs2sGpWSPc6R9HPg1+n6+4E55Qspe4XkekyDOzSaWe2IiK9JOgo4IN1024YMx1ctijs0mplVi1KS64+TPOr7j3T9TuBHZYuoCrR1dNHSWE9dnSclMLPaIelcklkZf5Nu+rSk/SPizAzD2mDFQ/GZmVWLQZPriOggqdE7v9yzbVULj5lqZjXq7cBeEdENIOkS4D6SzokjzpjGeiQn12ZWXQYciq8Po2L687bOvDszmlmt2rhoeXxmUQwDSbQ25mjzUHxmVkXWN4McFXUSbrk2sxp1LnCfpFtJ7ucHAKdnG9LQtDTlaOtYm3UYZmY91jeD/FlZoqgySXLtzoxmVlsi4jJJtwH7ppu+EBFLMwxpyFqa6j2JjJlVlUHLQiT9qrAcET/qva0WtXV0uSzEzGpSRDwbEbOALUZ6Yg3JiCEeLcTMqkkpNde7Fq9IqqeGJ5ABl4WY2ajw71kHMBxamnLu0GhmVaXf5FrSGZJWAXtIejl9rQKWA9dULMIMOLk2s1GgJvrQuOXazKpNv8l1RJwbEWOB8yJiXPoaGxGbRsQZFYyx4to6PFqImdUeSdsUrb6zj20jjpNrM6s2pWSQf5Z0QO+NEXFHGeLJXL6rm458Ny2NTq7NrOb8EdgHICIWp9uuYASX+rksxMyqTSkZ5H8WLTeTzO51D3BIWSLKWKHXuUcLMbNaIWlnkv4z4yUdW7RrHMl9fcRKkmuPFmJm1aOUGRrfWbwuaQrwvbJFlLG2zqQFxGUhZlZDdgLeQTKBTPE9fRXw0UwiGiatTfV0dnXTme+mMbe+86KZmQ2/DckgFwO7DHcg1aLweNEdGs2sVkTENcA1kvaLiP/LOp7hVLhXt3fkacw1ZhyNmVkJybWk7wORrtYBewH3ljOoLBU6xrjl2sxq0CmSXtNSHREfySKY4VC4V7d15Nmkxcm1mWWvlAxyTtFyHrgsIv5Wpngy55ZrM6th1xUtNwPHAM9kFMuwKE6uzcyqQSk115dIagR2TDc9XOrFJR0BXADUAz+PiG/12r81cBEwEVgJfKCoBzuSxgELgKsj4tRS33co1gT96mAAACAASURBVCXX7tBoZrUlIv5YvC7pMuCvGYUzLIrLQszMqkEp058fBDwK/BD4EfBIX0Pz9XFefXrOkcB04ARJ03sd9h3g0ojYAzgHOLfX/q8BFR3yry3tde6yEDMbBXYANss6iKFoccu1mVWZUjLI7wKHR8TDAJJ2BC5j8HFRZwKLIuLx9LzLgaNJWqILpgOfS5dvBa4u7JD0emBz4C/AjBLiHBYuCzGzWpXOshskszMGsBT4QqZBDVFrT8u1h+Mzs+pQyrhFDYXEGiAiHgEaSjhvEvB00fridFuxB4DCmKvHAGMlbSqpjiSpP62E9xlW7tBoZrUqnWV3XNHXHXuXiow0rc0uCzGz6lJSh0ZJPwd+na6/n1d3chyK04AfSDqRpPxjCdAFfAK4PiIWS+r3ZEmnAKcATJ06dVgCau/IU18nmjxeqpnVIElHAYXSvtsi4rqBjq92relsuqucXJtZlSgluf448EngP9L1O0lqrwezBJhStD453dYjIp4hbbmW1AocFxEvStoPeIukTwCtQKOktog4vdf5FwIXAsyYMSMYBu0deVoa6xkoqTczG4kkfQvYF/hNuunTkvaPiDMzDGtICp3P3XJtZtWilNFCOoDz09f6mA3sIGkbkqT6eOB9xQdImgCsjIhu4AySkUOIiPcXHXMiMKN3Yl0ubR1dLgkxs1r1NmCv9J6LpEuA+4ARm1zn6utoytU5uTazqtFv7YOkayW9U9Jr6qslbSvpHEn9TjwQEXngVOAGYCHw+4iYn553VHrYQcDDkh4h6bz4jSF8L8OivSPvzoxmVss2LloeX8oJki6StFzSvH72HyTpJUn3p6+zhyXSErU25TxaiJlVjYGyyI+SjOTxPUkrgRUkkw5sAywCfpBOqduviLgeuL7XtrOLlq8ArhjkGhcDFw90zHBq73RybWY161zgPkm3kowYcgBQylPBi4EfAJcOcMydEfGOIUe4AVqbc265NrOq0W8WGRFLgf8H/D9J04AtgVeARyJidUWiy0BbR95lIWZWkyLiMkm3kdRdA3whvdcPdt4d6b8DVaml0S3XZlY9SsoiI+IJ4ImyRlIl2jvybDa2KeswzMyGjaRp6X2ciHgWmNVrv4BJxTPkboD9JD1AMp36aRExv59Yhn2UJ5eFmFk1cRNtL+0dXS4LMbNac146f8A1wD2sK/PbHjgYeCvwZZL5CDbEvcDWEdEm6W0kE4Lt0NeB5RjlqaWpnufaOofjUmZmQ+YssheXhZhZrYmI90iaTjJPwUdIyvxWk3Q2vx74RkSsGcL1Xy5avl7SjyRNiIjnhhh6SVqacjz5fM1WK5rZCDNoFinpncCfCkM31bKI8GghZlaTImIB8MVyXFvSFsCyiAhJM0lGonq+HO/Vl7HNLgsxs+pRShb5XpIRQ/4IXBQRD5U5psx05LvJd4dbrs3Miki6jGTo1AmSFpOUkDQARMRPgHcDH5eUJ+n4fnxEDEvJRyncodHMqkkpk8h8QNI44ATgYkkB/BK4LCJWlTvASioM5dTSWJ9xJGZm1SMiThhk/w9IhurLREtTjtWdXXR3B3V1nl3XzLLV7yQyxdJ6uiuAy0lq9Y4B7pX0qTLGVnHtHV0ALgsxMxtBCk8b2zvdem1m2Rs0uZZ0lKSrgNtIHgPOjIgjgT2Bz5c3vMoqPFZ0WYiZ1SJJb5LUki5/QNL5krbOOq6hKjSIFBpIzMyyVErL9XHAf0fE7hFxXkQsB0gnkjm5rNFVWKHVwy3XZlajfgysllRoHHmMgWddHBFampJSPtddm1k1KCW5/gpwd2FF0kaFmboi4uayRJWRwo3ZybWZ1ah82tHwaOAHEfFDYGzGMQ3Z2Obknu3k2syqQSnJ9R+A4mH4utJtNafdZSFmVttWSToD+ADwp3RimYaMYxqylsZCWYiTazPLXinJdS4ieqa+SpcbyxdSdnpGC2nyaCFmVpPeC3QAJ0fEUmAycF62IQ1d4WmjW67NrBqUklyvkHRUYUXS0UBFZt2qtLa0M4xbrs2sRq0CLoiIOyXtCOwFXJZxTEPWM1qIk2szqwKlJNf/Dpwp6SlJTwNfAD5W3rCy0e6aazOrbXcATZImATcCHwQuzjSiYdDi5NrMqkgpk8g8BrxRUmu63lb2qDLS3pGnMVdHQ31Jw3+bmY00iojVkk4GfhQR/yXpgayDGqp1HRo9FJ+ZZa+kJlpJbwd2BZqlZPariDinjHFloq0j75IQM6tlkrQf8H7WDaU64lsTmnJ11NeJto61WYdiZlbSJDI/IekE8ylAwHuAET/pQF/aO/LuzGhmtewzwBnAVRExX9K2wK0ZxzRkkmhprPckMmZWFUpppt0/IvaQ9GBEfFXSd4E/lzuwLLR1dPUM6WRmVmsi4nbgdkmtkloj4nHgP7KOazi0NuU8WoiZVYVSHgeuSb+ulrQVsBbYsnwhZafdZSFmVsMk7S7pPmA+sEDSPZJ2zTqu4dDSlHOHRjOrCqUk19dK2phkLNR7gSeA35YzqKy0d+Y9UoiZ1bKfAp+LiK0jYirJFOg/yzimYdHa7JZrM6sOA2aS6exdN0fEi8AfJV0HNEfESxWJrsLaOvJM2WRM1mGYmZVLS0T01FhHxG2SWrIMaLi4LMTMqsWALdcR0Q38sGi9o1YTa3CHRjOreY9L+pKkaenrLODxrIMaDi2NLgsxs+pQSlnIzZKOU2EMvhrW3tHlshAzq2UfASYCV6aviem2ES+pufZoIWaWvVIyyY8BnwPyktaQDMcXETGurJFVWETQ3ukOjWZWuyLiBWpkdJDeWpvqXRZiZlWhlBkax1YikKyt7uwiwlOfm1ntkXQtEP3tj4ijKhhOWRRGC4kIRsGDVjOrYoNmkpIO6Gt7RNwx/OFkp1Cr5+TazGrQd7IOoNxam3Pku4OOfDfNDe47Y2bZKSWT/M+i5WZgJnAPcEhZIspI4XFiqzs0mlmNSSePqWmFkr62jryTazPLVCllIe8sXpc0BfheKReXdARwAVAP/DwivtVr/9bARSSdalYCH4iIxen2q0g6XDYA34+In5Tynhuq0BHGMzSaWa2SNJfXloe8BMwBvh4Rz1c+quFRuHe3d+SZ0NqUcTRmNpptSCa5GNhlsIMk1ZMM43dYes5sSbMiYkHRYd8BLo2ISyQdApwLfBB4FtgvIjoktQLz0nOf2YB4S7Ku5drJtZnVrD8DXaybCOx4YAywFLgYeGffp1W/lqKWazOzLJVSc/191rV01AF7kczUOJiZwKKIeDy9zuXA0UBxcj2dZCQSgFuBqwEiorPomCZKGzJwSFxzbWajwKERsU/R+lxJ90bEPpI+0N9Jki4C3gEsj4jd+tgvkqeUbwNWAydGRCn/TgybQsOIh+Mzs6yVkrTOIamxvgf4P+ALEdHvTbjIJODpovXF6bZiDwDHpsvHAGMlbQpJ+YmkB9NrfLuvVmtJp0iaI2nOihUrSgipf+2dTq7NrObVS5pZWJG0L0nZHsBATb4XA0cMsP9IYIf0dQrw46GFuf5amwst12sr/dZmZq9SSiZ5BbAmIrogKfeQNCYiVg/D+58G/EDSicAdwBKSR5ZExNPAHpK2Aq6WdEVELCs+OSIuBC4EmDFjRr/DTJXCZSFmNgr8G3BRWm4n4GXg5HQK9HP7Oyki7pA0bYDrHk1S4hfAPyRtLGnLiHh2+EIfWKEzeptbrs0sY6VkkjcDhwJt6fpGwI3A/oOctwSYUrQ+Od3WI22NPhYgvdkfFxEv9j5G0jzgLSSJflmsKwtxL3Mzq00RMRvYXdL4dP2lot2/H8Kl+3tS+ZrkWtIpJK3bTJ06dQhv+WotTes6NJqZZamUspDmiCgk1qTLY0o4bzawg6RtJDWSdJyZVXyApAmSCjGcQTJyCJImS9ooXd4EeDPwcAnvucHaPFqImdU4SeMlnU/SaHKzpO8WEu1KiYgLI2JGRMyYOHHisF3XybWZVYtSkut2ST0dYCS9HnhlsJMiIg+cCtwALAR+HxHzJZ0jqTAb2EHAw5IeATYHvpFu3wW4S9IDwO3AdyJibonf0wZp78gzprGeujrP7GVmNesiYBXwr+nrZeCXw3DdQZ9UlluhYcSjhZhZ1kpppv0M8AdJz5DU6G0BvLeUi0fE9cD1vbadXbR8BX2UekTETcAepbzHcGnvyLszo5nVuu0i4rii9a9Kun8YrjsLODUdFeoNwEuVrLcGqK8TYxrr3XJtZpkrZRKZ2ZJ2BnZKNz0cETXXHbutI+/OjGZW616R9OaI+CuApDdRwpNISZeRPGmcIGkx8GWSCb5IJ/i6nmQYvkUkQ/GdVJboB9HSlHPLtZllrpRxrj8J/CYi5qXrm0g6ISJ+VPboKihpuXZnRjOraf8OXFpUZ/0C8OHBToqIEwbZH8Anhx7e0LQ25TxaiJllrpSa648Wj+ARES8AHy1fSNlo7+hyZ0Yzq2kR8UBE7ElSdrdHROwNHJJxWMOmpcllIWaWvVKS6/p09i2gZ1rzxvKFlA2XhZjZaBERL0fEy+nq5wY8eARpaXRZiJllr5Tk+i/A7yS9VdJbgcvSbTWlvdMdGs1sVKqZIZLGNufccm1mmSslm/wCyYD/H0/XbwJ+VraIMuLRQsxslBrS7LbVxB0azawalDJaSDfwk/SFpLcA36cKOq8Mp6QsxB0azaz2SFpF30m0SGbdrQktTW65NrPsldRUK2lv4ASSSQf+CVxZzqAqLd/VzZq13W65NrOaFBFjs46hElrdcm1mVaDfbFLSjiQJ9QnAc8DvAEXEwRWKrWLaO5Ohm9yh0cxs5GppzLFmbTf5rm5y9aV0KTIzG34DZZMPAXcC74iIRQCSPluRqCqs8BjRLddmZiNXYa6C9s4uxm/k5NrMsjHQ3edY4FngVkk/S0cKqZle5cWcXJuZjXxjm5N7uEtDzCxL/SbXEXF1RBwP7AzcCnwG2EzSjyUdXqkAK6FwI3aHRjOzkavQQOJOjWaWpUGfm0VEe0T8NiLeCUwG7iMZnq9mtKfT5XqGRjOzkauQXLvl2syytF5FaRHxQkRcGBFvLVdAWWhzWYiZ2YjX6pZrM6sC7vHBuhuxRwsxMxu5Ck8fnVybWZacXJNMfQ5uuTYzG8kKHRpXrXFybWbZcXJNcYdGJ9dmZiOVOzSaWTVwck1yI64TNDf44zAzG6mKx7k2M8uKs0mS0UJamnJINTmMt5nZqNCUq6ehXh4txMwy5eSapCzEJSFmZiNfS1POZSFmlikn1yRlIe7MaGY28rU25dxybWaZcnJN0nLt5NrMbORrbcrR5tFCzCxDTq5JWq499bmZ2cjX0pTrGV7VzCwLTq5JOzR66nMzsxGvpSlHW4dHCzGz7Di5xh0azcxqRWtTvTs0mlmmnFyTzNDommszs75JOkLSw5IWSTq9j/0nSloh6f709W9ZxAlJzbWTazPLkjNKPFqImVl/JNUDPwQOAxYDsyXNiogFvQ79XUScWvEAe2lxh0Yzy1hZW65LaO3YWtLNkh6UdJukyen2vST9n6T56b73livGjnwXa7vCHRrNzPo2E1gUEY9HRCdwOXB0xjH1qzXt0BgRWYdiZqNU2ZLrotaOI4HpwAmSpvc67DvApRGxB3AOcG66fTXwoYjYFTgC+J6kjcsRZ3va8cUt12ZmfZoEPF20vjjd1ttxaWPIFZKmVCa012ppytEd8Mpad2o0s2yUs+W6lNaO6cAt6fKthf0R8UhEPJouPwMsByaWI8hCbZ6TazOzDXYtMC1tKLkJuKS/AyWdImmOpDkrVqwY9kAK93JPJGNmWSlncl1Ka8cDwLHp8jHAWEmbFh8gaSbQCDxWjiALN2CPFmJm1qclQHFL9OR0W4+IeD4iOtLVnwOv7+9iEXFhRMyIiBkTJw5/m0mhxK/dw/GZWUayHi3kNOBASfcBB5LcsHvuiJK2BH4FnBQR3b1PHo4WELdcm5kNaDawg6RtJDUCxwOzig9I79UFRwELKxjfq7Q2NQC4U6OZZaacGWUprR3PkLZcS2oFjouIF9P1ccCfgC9GxD/6eoOIuBC4EGDGjBkb1HtlXcu1OzSamfUWEXlJpwI3APXARRExX9I5wJyImAX8h6SjgDywEjgxq3hb0nu5y0LMLCvlTK57WjtIkurjgfcVHyBpArAybZU+A7go3d4IXEXS2fGKMsboDo1mZoOIiOuB63ttO7to+QySe3jmCiV+HuvazLJStrKQiMgDhdaOhcDvC60daQsHwEHAw5IeATYHvpFu/1fgAODEokkJ9ipHnD1lIZ7+3MxsxCs0lLR3Ork2s2yUNaMsobXjCuA1LdMR8Wvg1+WMrcAdGs3MakerRwsxs4xl3aExc+7QaGZWO3qSa3doNLOMjPrkuq0zT2N9HY25Uf9RmJmNeGMa66kTPLx0lWdpNLNMjPqMsr0j39O73MzMRjZJvO8NU7nyviV86Zp5dHU7wTazyhr1tRDtHV0uCTEzqyFfO3o3Wpsa+Mntj/F8Wyf//d69aG5wI4qZVcaozyrbOvLuzGhmVkMkcfqROzNxbBNfu24BK9vv5mcfnsG45oasQzOzUcBlIR15t1ybmdWgk9+8DRccvxf3PPkC7/3pP1j+8pqsQzKzUcDJtZNrM7OadfRek7joxH158vl2jv3x3/nnc+1Zh2RmNW7UJ9dJWYhr8czMatUBO07kso++kdWdXbz7x3/nwcUvZh2SmdWwUZ9ct3d0eXZGM7Mat+eUjbni3/djo8Z6jr/wH9zxyIqsQzKzGuXk2mUhZmajwrYTW7ny4/uz9aYtfOTi2Vxz/5KsQzKzGjSqk+uIoL3To4WYmY0Wm41r5ncfeyOv33oTPvu7+51gm9mwG9XJ9Stru+gOT31uZjaajGtu4OKTZrLvtNfx2d/dz3UPPpN1SGZWQ0Z1ct3WkQdwh0Yzs1Fmo8Z6LjpxX16/9SZ8+vL7+cu8Z7MOycxqxKhOrts7ugC3XJuZjUYtTTl+edJM9pw8nlN/ex83zl+adUhmVgNGeXKdtFw7uTYzG51am3Jc8pGZ7DZpPJ/87b3c8tCyrEMysxFuVCfX68pCnFybmY1WY5sbuOQjM9lly3H8+6/u5XYP02dmQzCqk2u3XJuZGcD4jRq49CMz2WHzVj566Rz++uhzWYdkZiPUqE6u3aHRzMwKNh7TyK9PfgPbTmjh5Etm8/fHnGCb2fob1U227tBoZmbFNmlp5Df/9gbe97O7OPniOfz4A/uw0xZj6Q7o7g66I+jqjmQ91q135LtZs7aLjrXp13R9zdou1uS76VjbTa5ejGmsZ6OGejZqrGdMY44xjfU0N9QzpjF55err6O5OrtkVQUTQ1U36ntHztTsAoieuIImHoCe23l67BRrqxZjGXBJTQz3NjXU96/V1KutnHRGkIdPds5x8Lf6co9dyd3pcb+K18UrF++lzpU5KX6D0a2Gb0uXCdQqnKd2wbv3V2210G9VZpctCzMyst01bm/jNR9/A8Rf+gxN/OTvrcDLTWF9Hc0Mdjbk6irPR/hLW4j84uruTJLkr1v0xUEiMC4l0LVJPYp4k2mLdep0EyX/JvqJlKCwnS4V9dX0dV7gW6/7IKHyevT/X6LVhKB/7YH82rO+1e39P6/6ASb7n4j+21n1/yXdcWC/+bOrqknN7PvvizymK/hyLdbFGBI25Om787IHrGf3ARnVWOeV1Y/iXXTenpXFUfwxmZtbLhNYm/vCx/bhxwVK6A+rTf6zr69JWzrrkH/Fku2hqqKM5V09zQx3NDfU05ZKvyauOplw9a7uS1uzVncnrlc4uVnfmeWVtYbmLfHc3dRL1dVr3XhL1dfRs792iui4BS5O4OhUlav2LgM40plc6u5M41nbxSme+Z33N2i46u7pfdU7RFV61vfdnUkgq6+vUk+zU69XJYk/8FBIsvSoZLVyz93Lvb62vxK441ugV66vOi+InEYU/Aoq2dfdKYHtdp3Dt7uj7WkHhqce6Jwq9k8SeJLJn27qksrvo2KDwdCJe03pOUXIKxcnqq21I43qpfwyVeu3i7zWKst1kW/L99vyOQPpHSfHvTrIeFD7ndT+35EnIunWt+2Be9eShsNxQP/wV0qM6qzxity04Yrctsg7DzMyq0CYtjbx336nDdr36uiTZ3njMsF3SzKrQqO7QaGZmpZF0hKSHJS2SdHof+5sk/S7df5ekaZWP0swse06uzcxsQJLqgR8CRwLTgRMkTe912MnACxGxPfDfwLcrG6WZWXVwcm1mZoOZCSyKiMcjohO4HDi61zFHA5eky1cAb5WHTjCzUcjJtZmZDWYS8HTR+uJ0W5/HREQeeAnYtPeFJJ0iaY6kOStWeCZEM6s9Tq7NzKxiIuLCiJgRETMmTpyYdThmZsPOybWZmQ1mCTClaH1yuq3PYyTlgPHA8xWJzsysiji5NjOzwcwGdpC0jaRG4HhgVq9jZgEfTpffDdwSvWewMDMbBUb1ONdmZja4iMhLOhW4AagHLoqI+ZLOAeZExCzgF8CvJC0CVpIk4GZmo45qpWFB0grgyfU8bQLwXBnCqYSRGrvjrqyRGjeM3Ng3NO6tI2JUFSGPsvu2466skRo3jNzYR1vc/d6zaya53hCS5kTEjKzj2BAjNXbHXVkjNW4YubGP1LhHipH6+TruyhqpccPIjd1xr+OaazMzMzOzYeLk2szMzMxsmIz25PrCrAMYgpEau+OurJEaN4zc2Edq3CPFSP18HXdljdS4YeTG7rhTo7rm2szMzMxsOI32lmszMzMzs2EzapNrSUdIeljSIkmnZx3PQCQ9IWmupPslzUm3vU7STZIeTb9uknWcAJIukrRc0ryibX3GqsT/pD+DByXtU2Vxf0XSkvRzv1/S24r2nZHG/bCkf8kmapA0RdKtkhZImi/p0+n2qv7MB4i7qj9zSc2S7pb0QBr3V9Pt20i6K43vd+lEK0hqStcXpfunZRF3LfA9uzx8z64s37Mzib3y9+2IGHUvkkkQHgO2BRqBB4DpWcc1QLxPABN6bfsv4PR0+XTg21nHmcZyALAPMG+wWIG3AX8GBLwRuKvK4v4KcFofx05Pf2eagG3S36X6jOLeEtgnXR4LPJLGV9Wf+QBxV/Vnnn5urelyA3BX+jn+Hjg+3f4T4OPp8ieAn6TLxwO/y+LzHukv37PLGqvv2ZWN2/fsysde8fv2aG25ngksiojHI6ITuBw4OuOY1tfRwCXp8iXAuzKMpUdE3EEyO1ux/mI9Grg0Ev8ANpa0ZWUifbV+4u7P0cDlEdEREf8EFpH8TlVcRDwbEfemy6uAhcAkqvwzHyDu/lTFZ55+bm3pakP6CuAQ4Ip0e+/Pu/BzuAJ4qyRVKNxa4nt2mfieXVm+Z1deFvft0ZpcTwKeLlpfzMC/JFkL4EZJ90g6Jd22eUQ8my4vBTbPJrSS9BfrSPg5nJo+iruo6DFuVcadPrram+Sv8hHzmfeKG6r8M5dUL+l+YDlwE0mLzIsRke8jtp640/0vAZtWNuKaUDU//xL5np2dqr5/FPM9u3Iqfd8ercn1SPPmiNgHOBL4pKQDindG8uxiRAz7MpJiBX4MbAfsBTwLfDfbcPonqRX4I/CZiHi5eF81f+Z9xF31n3lEdEXEXsBkkpaYnTMOyaqP79nZqPr7R4Hv2ZVV6fv2aE2ulwBTitYnp9uqUkQsSb8uB64i+cVYVng0lH5dnl2Eg+ov1qr+OUTEsvR/yG7gZ6x7pFVVcUtqILnZ/SYirkw3V/1n3lfcI+UzB4iIF4Fbgf1IHtXm0l3FsfXEne4fDzxf4VBrQdX9/Afie3Y2Rsr9w/fs7FTqvj1ak+vZwA5pT9FGkoL1WRnH1CdJLZLGFpaBw4F5JPF+OD3sw8A12URYkv5inQV8KO0N/UbgpaLHYpnrVdd2DMnnDkncx6c9ircBdgDurnR8kPQkB34BLIyI84t2VfVn3l/c1f6ZS5ooaeN0eSPgMJLaw1uBd6eH9f68Cz+HdwO3pK1Stn58z66sqr5/9Kfa7x/ge3al4i2WyX27dw/H0fIi6YH7CEndzRezjmeAOLcl6XH7ADC/ECtJ/c/NwKPA/wKvyzrWNK7LSB4NrSWpYTq5v1hJevD+MP0ZzAVmVFncv0rjejD9n23LouO/mMb9MHBkhnG/meTx4YPA/enrbdX+mQ8Qd1V/5sAewH1pfPOAs9Pt25L8w7EI+APQlG5vTtcXpfu3zep3ZaS/fM8uW7y+Z1c2bt+zKx97xe/bnqHRzMzMzGyYjNayEDMzMzOzYefk2szMzMxsmDi5NjMzMzMbJk6uzczMzMyGiZNrMzMzM7Nh4uTaqoakkPTdovXTJH1lmK59saR3D37kkN/nPZIWSrq11/Zpkt5X7vc3M6sU37PN+ubk2qpJB3CspAlZB1KsaAanUpwMfDQiDu61fRrQ5416Pa9vZlYtfM8264OTa6smeeBC4LO9d/RuxZDUln49SNLtkq6R9Likb0l6v6S7Jc2VtF3RZQ6VNEfSI5LekZ5fL+k8SbMlPSjpY0XXvVPSLGBBH/GckF5/nqRvp9vOJhlo/xeSzut1yreAt0i6X9JnJZ0oaZakW4Cb01ndLkrjvk/S0YPEt6WkO9LrzZP0lg38zM3MNpTv2b5nWx/815dVmx8CD0r6r/U4Z09gF2Al8Djw84iYKenTwKeAz6THTQNmAtsBt0raHvgQyXSy+0pqAv4m6cb0+H2A3SLin8VvJmkr4NvA64EXgBslvSsizpF0CHBaRMzpFePp6fbCPxAnptffIyJWSvomyRSrH1EyTevdkv4XeH8/8R0L3BAR35BUD4xZj8/LzGy4+J7te7b14uTaqkpEvCzpUuA/gFdKPG12RDwLIOkxoHCjnQsUP+r7fUR0A49KehzYGTgc2KOohWU8sAPQCdzd+yad2he4LSJWpO/5G+AA4OoS4y24KSJWpsuHA0dJOi1dbwamDhDfbOAiSQ3A1RFx/3q+ddISwgAAAbtJREFUt5nZkPme7Xu2vZaTa6tG3wPuBX5ZtC1PWsYkqQ5oLNrXUbTcXbTezat/x6PX+wQg4FMRcUPxDkkHAe0bFn7Jiq8v4LiIeLhXHH3Gl+47AHg7cPH/b9+OVeKKoigM/ytgJUmVXrCw9wniA4jYWOQNFEwppLeOL6CFWNpNIcRK0qYIaGelpAnBMimEEHaKc4KjEQeGG7X4v+7OMGd2tdjss2+Snao6+K/VStL9zOybOsxsuXOt56dPBg5pL5r8dUm70gNYAWamOHotyYu+0zcPnAPHwEafJpBkIcnshHM+A2+SvO7Xe2+BTxN+8wN4+cD3x8C7HswkWRz7/J/6kswB36tqF9ijXVdK0qMzs81s3ebkWs/VB2Bz7HkXGCU5BT4y3YTiKy1kXwHrVXWdZI+21/elh+QVsPrQIVX1Lcl74IQ2vTiqqtGE/z4Dfvf692l7f+O2adOfsz7luQCWaSF8X31LwFaSX8BP2h6iJD0VM9vMVpequ7cukiRJkqbhWogkSZI0EJtrSZIkaSA215IkSdJAbK4lSZKkgdhcS5IkSQOxuZYkSZIGYnMtSZIkDcTmWpIkSRrIH3FhX9ENLDCvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrain the model with a different learning algorithm**\n",
        "\n"
      ],
      "metadata": {
        "id": "CorYYWR5OAXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfdf.keras.get_all_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNr9NajNNfuV",
        "outputId": "a27c695f-0c90-49c8-b33a-bbe6eaae1dd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensorflow_decision_forests.keras.RandomForestModel,\n",
              " tensorflow_decision_forests.keras.GradientBoostedTreesModel,\n",
              " tensorflow_decision_forests.keras.CartModel,\n",
              " tensorflow_decision_forests.keras.DistributedGradientBoostedTreesModel]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# help works anywhere.\n",
        "help(tfdf.keras.RandomForestModel)\n",
        "\n",
        "# ? only works in ipython or notebooks, it usually opens on a separate panel.\n",
        "tfdf.keras.RandomForestModel?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb-vqMLuOQs3",
        "outputId": "045883f4-bdd7-4886-ee3a-4269e50b91a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class RandomForestModel in module tensorflow_decision_forests.keras:\n",
            "\n",
            "class RandomForestModel(tensorflow_decision_forests.keras.wrappers.RandomForestModel)\n",
            " |  RandomForestModel(*args, **kwargs)\n",
            " |  \n",
            " |  Random Forest learning algorithm.\n",
            " |  \n",
            " |  A Random Forest (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n",
            " |  is a collection of deep CART decision trees trained independently and without\n",
            " |  pruning. Each tree is trained on a random subset of the original training \n",
            " |  dataset (sampled with replacement).\n",
            " |  \n",
            " |  The algorithm is unique in that it is robust to overfitting, even in extreme\n",
            " |  cases e.g. when there is more features than training examples.\n",
            " |  \n",
            " |  It is probably the most well-known of the Decision Forest training\n",
            " |  algorithms.\n",
            " |  \n",
            " |  Usage example:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow_decision_forests as tfdf\n",
            " |  import pandas as pd\n",
            " |  \n",
            " |  dataset = pd.read_csv(\"project/dataset.csv\")\n",
            " |  tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n",
            " |  \n",
            " |  model = tfdf.keras.RandomForestModel()\n",
            " |  model.fit(tf_dataset)\n",
            " |  \n",
            " |  print(model.summary())\n",
            " |  ```\n",
            " |  \n",
            " |  Attributes:\n",
            " |    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,\n",
            " |      Task.RANKING, Task.CATEGORICAL_UPLIFT).\n",
            " |    features: Specify the list and semantic of the input features of the model.\n",
            " |      If not specified, all the available features will be used. If specified\n",
            " |      and if `exclude_non_specified_features=True`, only the features in\n",
            " |      `features` will be used by the model. If \"preprocessing\" is used,\n",
            " |      `features` corresponds to the output of the preprocessing. In this case,\n",
            " |      it is recommended for the preprocessing to return a dictionary of tensors.\n",
            " |    exclude_non_specified_features: If true, only use the features specified in\n",
            " |      `features`.\n",
            " |    preprocessing: Functional keras model or @tf.function to apply on the input\n",
            " |      feature before the model to train. This preprocessing model can consume\n",
            " |      and return tensors, list of tensors or dictionary of tensors. If\n",
            " |      specified, the model only \"sees\" the output of the preprocessing (and not\n",
            " |      the raw input). Can be used to prepare the features or to stack multiple\n",
            " |      models on top of each other. Unlike preprocessing done in the tf.dataset,\n",
            " |      the operation in \"preprocessing\" are serialized with the model.\n",
            " |    postprocessing: Like \"preprocessing\" but applied on the model output.\n",
            " |    ranking_group: Only for `task=Task.RANKING`. Name of a tf.string feature that\n",
            " |      identifies queries in a query/document ranking task. The ranking group\n",
            " |      is not added automatically for the set of features if\n",
            " |      `exclude_non_specified_features=false`.\n",
            " |    uplift_treatment: Only for task=Task.CATEGORICAL_UPLIFT. Name of an integer\n",
            " |      feature that identifies the treatment in an uplift problem. The value 0 is\n",
            " |      reserved for the control treatment.\n",
            " |    temp_directory: Temporary directory used to store the model Assets after the\n",
            " |      training, and possibly as a work directory during the training. This\n",
            " |      temporary directory is necessary for the model to be exported after\n",
            " |      training e.g. `model.save(path)`. If not specified, `temp_directory` is\n",
            " |      set to a temporary directory using `tempfile.TemporaryDirectory`. This\n",
            " |      directory is deleted when the model python object is garbage-collected.\n",
            " |    verbose: Verbosity mode. 0 = silent, 1 = small details, 2 = full details.\n",
            " |    hyperparameter_template: Override the default value of the hyper-parameters.\n",
            " |      If None (default) the default parameters of the library are used. If set,\n",
            " |      `default_hyperparameter_template` refers to one of the following\n",
            " |      preconfigured hyper-parameter sets. Those sets outperforms the default\n",
            " |      hyper-parameters (either generally or in specific scenarios).\n",
            " |      You can omit the version (e.g. remove \"@v5\") to use the last version of\n",
            " |      the template. In this case, the hyper-parameter can change in between\n",
            " |      releases (not recommended for training in production).\n",
            " |      - better_default@v1: A configuration that is generally better than the\n",
            " |        default parameters without being more expensive. The parameters are:\n",
            " |        winner_take_all=True.\n",
            " |      - benchmark_rank1@v1: Top ranking hyper-parameters on our benchmark\n",
            " |        slightly modified to run in reasonable time. The parameters are:\n",
            " |        winner_take_all=True, categorical_algorithm=\"RANDOM\",\n",
            " |        split_axis=\"SPARSE_OBLIQUE\", sparse_oblique_normalization=\"MIN_MAX\",\n",
            " |        sparse_oblique_num_projections_exponent=1.0.\n",
            " |  \n",
            " |    advanced_arguments: Advanced control of the model that most users won't need\n",
            " |      to use. See `AdvancedArguments` for details.\n",
            " |    num_threads: Number of threads used to train the model. Different learning\n",
            " |      algorithms use multi-threading differently and with different degree of\n",
            " |      efficiency. If `None`, `num_threads` will be automatically set to the\n",
            " |      number of processors (up to a maximum of 32; or set to 6 if the number of\n",
            " |      processors is not available).\n",
            " |      Making `num_threads` significantly larger than the number of processors\n",
            " |      can slow-down the training speed. The default value logic might change in\n",
            " |      the future.\n",
            " |    name: The name of the model.\n",
            " |    max_vocab_count: Default maximum size of the vocabulary for CATEGORICAL and\n",
            " |      CATEGORICAL_SET features stored as strings. If more unique values exist,\n",
            " |      only the most frequent values are kept, and the remaining values are\n",
            " |      considered as out-of-vocabulary. The value `max_vocab_count` defined in a\n",
            " |      `FeatureUsage` (if any) takes precedence.\n",
            " |    try_resume_training: If true, the model training resumes from the checkpoint\n",
            " |      stored in the `temp_directory` directory. If `temp_directory` does not\n",
            " |      contain any model checkpoint, the training start from the beginning.\n",
            " |      Resuming training is useful in the following situations: (1) The training\n",
            " |        was interrupted by the user (e.g. ctrl+c or \"stop\" button in a\n",
            " |        notebook). (2) the training job was interrupted (e.g. rescheduling), ond\n",
            " |        (3) the hyper-parameter of the model were changed such that an initially\n",
            " |        completed training is now incomplete (e.g. increasing the number of\n",
            " |        trees).\n",
            " |      Note: Training can only be resumed if the training datasets is exactly the\n",
            " |        same (i.e. no reshuffle in the tf.data.Dataset).\n",
            " |    check_dataset: If set to true, test if the dataset is well configured for\n",
            " |      the training: (1) Check if the dataset does contains any `repeat`\n",
            " |        operations, (2) Check if the dataset does contain a `batch` operation,\n",
            " |        (3) Check if the dataset has a large enough batch size (min 100 if the\n",
            " |        dataset contains more than 1k examples or if the number of examples is\n",
            " |        not available) If set to false, do not run any test.\n",
            " |    adapt_bootstrap_size_ratio_for_maximum_training_duration: Control how the\n",
            " |      maximum training duration (if set) is applied. If false, the training\n",
            " |      stop when the time is used. If true, adapts the size of the sampled\n",
            " |      dataset used to train each tree such that `num_trees` will train within\n",
            " |      `maximum_training_duration`. Has no effect if there is no maximum\n",
            " |      training duration specified. Default: False.\n",
            " |    allow_na_conditions: If true, the tree training evaluates conditions of the\n",
            " |      type `X is NA` i.e. `X is missing`. Default: False.\n",
            " |    bootstrap_size_ratio: Number of examples used to train each trees;\n",
            " |      expressed as a ratio of the training dataset size. Default: 1.0.\n",
            " |    bootstrap_training_dataset: If true (default), each tree is trained on a\n",
            " |      separate dataset sampled with replacement from the original dataset. If\n",
            " |      false, all the trees are trained on the entire same dataset. If\n",
            " |      bootstrap_training_dataset:false, OOB metrics are not available.\n",
            " |      bootstrap_training_dataset=false is used in \"Extremely randomized trees\"\n",
            " |      (https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf).\n",
            " |      Default: True.\n",
            " |    categorical_algorithm: How to learn splits on categorical attributes.\n",
            " |      - `CART`: CART algorithm. Find categorical splits of the form \"value \\\\in\n",
            " |        mask\". The solution is exact for binary classification, regression and\n",
            " |        ranking. It is approximated for multi-class classification. This is a\n",
            " |        good first algorithm to use. In case of overfitting (very small\n",
            " |        dataset, large dictionary), the \"random\" algorithm is a good\n",
            " |        alternative.\n",
            " |      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the\n",
            " |        form \"attribute == param\". This method is similar (but more efficient)\n",
            " |        than converting converting each possible categorical value into a\n",
            " |        boolean feature. This method is available for comparison purpose and\n",
            " |        generally performs worse than other alternatives.\n",
            " |      - `RANDOM`: Best splits among a set of random candidate. Find the a\n",
            " |        categorical split of the form \"value \\\\in mask\" using a random search.\n",
            " |        This solution can be seen as an approximation of the CART algorithm.\n",
            " |        This method is a strong alternative to CART. This algorithm is inspired\n",
            " |        from section \"5.1 Categorical Variables\" of \"Random Forest\", 2001.\n",
            " |        Default: \"CART\".\n",
            " |    categorical_set_split_greedy_sampling: For categorical set splits e.g.\n",
            " |      texts. Probability for a categorical value to be a candidate for the\n",
            " |      positive set. The sampling is applied once per node (i.e. not at every\n",
            " |      step of the greedy optimization). Default: 0.1.\n",
            " |    categorical_set_split_max_num_items: For categorical set splits e.g. texts.\n",
            " |      Maximum number of items (prior to the sampling). If more items are\n",
            " |      available, the least frequent items are ignored. Changing this value is\n",
            " |      similar to change the \"max_vocab_count\" before loading the dataset, with\n",
            " |      the following exception: With `max_vocab_count`, all the remaining items\n",
            " |      are grouped in a special Out-of-vocabulary item. With `max_num_items`,\n",
            " |      this is not the case. Default: -1.\n",
            " |    categorical_set_split_min_item_frequency: For categorical set splits e.g.\n",
            " |      texts. Minimum number of occurrences of an item to be considered.\n",
            " |      Default: 1.\n",
            " |    compute_oob_performances: If true, compute the Out-of-bag evaluation (then\n",
            " |      available in the summary and model inspector). This evaluation is a cheap\n",
            " |      alternative to cross-validation evaluation. Default: True.\n",
            " |    compute_oob_variable_importances: If true, compute the Out-of-bag feature\n",
            " |      importance (then available in the summary and model inspector). Note that\n",
            " |      the OOB feature importance can be expensive to compute. Default: False.\n",
            " |    growing_strategy: How to grow the tree.\n",
            " |      - `LOCAL`: Each node is split independently of the other nodes. In other\n",
            " |        words, as long as a node satisfy the splits \"constraints (e.g. maximum\n",
            " |        depth, minimum number of observations), the node will be split. This is\n",
            " |        the \"classical\" way to grow decision trees.\n",
            " |      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all\n",
            " |        the nodes of the tree is selected for splitting. This method is also\n",
            " |        called \"best first\" or \"leaf-wise growth\". See \"Best-first decision\n",
            " |        tree learning\", Shi and \"Additive logistic regression : A statistical\n",
            " |        view of boosting\", Friedman for more details. Default: \"LOCAL\".\n",
            " |    honest: In honest trees, different training examples are used to infer the\n",
            " |      structure and the leaf values. This regularization technique trades\n",
            " |      examples for bias estimates. It might increase or reduce the quality of\n",
            " |      the model. See \"Generalized Random Forests\", Athey et al. In this paper,\n",
            " |      Honest tree are trained with the Random Forest algorithm with a sampling\n",
            " |      without replacement. Default: False.\n",
            " |    in_split_min_examples_check: Whether to check the `min_examples` constraint\n",
            " |      in the split search (i.e. splits leading to one child having less than\n",
            " |      `min_examples` examples are considered invalid) or before the split\n",
            " |      search (i.e. a node can be derived only if it contains more than\n",
            " |      `min_examples` examples). If false, there can be nodes with less than\n",
            " |      `min_examples` training examples. Default: True.\n",
            " |    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the\n",
            " |      distribution of the labels of the training examples) of non-leaf nodes.\n",
            " |      This information is not used during serving, however it can be used for\n",
            " |      model interpretation as well as hyper parameter tuning. This can take\n",
            " |      lots of space, sometimes accounting for half of the model size. Default:\n",
            " |      True.\n",
            " |    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees\n",
            " |      will be roots. Negative values are ignored. Default: 16.\n",
            " |    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable\n",
            " |      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.\n",
            " |      Default: None.\n",
            " |    maximum_model_size_in_memory_in_bytes: Limit the size of the model when\n",
            " |      stored in ram. Different algorithms can enforce this limit differently.\n",
            " |      Note that when models are compiled into an inference, the size of the\n",
            " |      inference engine is generally much smaller than the original model.\n",
            " |      Default: -1.0.\n",
            " |    maximum_training_duration_seconds: Maximum training duration of the model\n",
            " |      expressed in seconds. Each learning algorithm is free to use this\n",
            " |      parameter at it sees fit. Enabling maximum training duration makes the\n",
            " |      model training non-deterministic. Default: -1.0.\n",
            " |    min_examples: Minimum number of examples in a node. Default: 5.\n",
            " |    missing_value_policy: Method used to handle missing attribute values.\n",
            " |      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the\n",
            " |        mean (in case of numerical attribute) or the most-frequent-item (in\n",
            " |        case of categorical attribute) computed on the entire dataset (i.e. the\n",
            " |        information contained in the data spec).\n",
            " |      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean\n",
            " |        (numerical attribute) or most-frequent-item (in the case of categorical\n",
            " |        attribute) evaluated on the training examples in the current node.\n",
            " |      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from\n",
            " |        randomly sampled values from the training examples in the current node.\n",
            " |        This method was proposed by Clinic et al. in \"Random Survival Forests\"\n",
            " |        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).\n",
            " |        Default: \"GLOBAL_IMPUTATION\".\n",
            " |    num_candidate_attributes: Number of unique valid attributes tested for each\n",
            " |      node. An attribute is valid if it has at least a valid split. If\n",
            " |      `num_candidate_attributes=0`, the value is set to the classical default\n",
            " |      value for Random Forest: `sqrt(number of input attributes)` in case of\n",
            " |      classification and `number_of_input_attributes / 3` in case of\n",
            " |      regression. If `num_candidate_attributes=-1`, all the attributes are\n",
            " |      tested. Default: 0.\n",
            " |    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If\n",
            " |      set, it is equivalent to `num_candidate_attributes =\n",
            " |      number_of_input_features x num_candidate_attributes_ratio`. The possible\n",
            " |      values are between ]0, and 1] as well as -1. If not set or equal to -1,\n",
            " |      the `num_candidate_attributes` is used. Default: -1.0.\n",
            " |    num_oob_variable_importances_permutations: Number of time the dataset is\n",
            " |      re-shuffled to compute the permutation variable importances. Increasing\n",
            " |      this value increase the training time (if\n",
            " |      \"compute_oob_variable_importances:true\") as well as the stability of the\n",
            " |      oob variable importance metrics. Default: 1.\n",
            " |    num_trees: Number of individual decision trees. Increasing the number of\n",
            " |      trees can increase the quality of the model at the expense of size,\n",
            " |      training speed, and inference latency. Default: 300.\n",
            " |    random_seed: Random seed for the training of the model. Learners are\n",
            " |      expected to be deterministic by the random seed. Default: 123456.\n",
            " |    sampling_with_replacement: If true, the training examples are sampled with\n",
            " |      replacement. If false, the training samples are sampled without\n",
            " |      replacement. Only used when \"bootstrap_training_dataset=true\". If false\n",
            " |      (sampling without replacement) and if \"bootstrap_size_ratio=1\" (default),\n",
            " |      all the examples are used to train all the trees (you probably do not\n",
            " |      want that). Default: True.\n",
            " |    sorting_strategy: How are sorted the numerical features in order to find\n",
            " |      the splits\n",
            " |      - PRESORT: The features are pre-sorted at the start of the training. This\n",
            " |        solution is faster but consumes much more memory than IN_NODE.\n",
            " |      - IN_NODE: The features are sorted just before being used in the node.\n",
            " |        This solution is slow but consumes little amount of memory.\n",
            " |      . Default: \"PRESORT\".\n",
            " |    sparse_oblique_normalization: For sparse oblique splits i.e.\n",
            " |      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,\n",
            " |      before applying the sparse oblique projections.\n",
            " |      - `NONE`: No normalization.\n",
            " |      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard\n",
            " |        deviation on the entire train dataset. Also known as Z-Score\n",
            " |        normalization.\n",
            " |      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated\n",
            " |        on the entire train dataset. Default: None.\n",
            " |    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.\n",
            " |      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections\n",
            " |      to test at each node as `num_features^num_projections_exponent`. Default:\n",
            " |      None.\n",
            " |    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.\n",
            " |      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections\n",
            " |      to test at each node as `num_features^num_projections_exponent`. Default:\n",
            " |      None.\n",
            " |    sparse_oblique_weights: For sparse oblique splits i.e.\n",
            " |      `split_axis=SPARSE_OBLIQUE`. Possible values:\n",
            " |      - `BINARY`: The oblique weights are sampled in {-1,1} (default).\n",
            " |      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:\n",
            " |        None.\n",
            " |    split_axis: What structure of split to consider for numerical features.\n",
            " |      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).\n",
            " |        This is the \"classical\" way to train a tree. Default value.\n",
            " |      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. splits one a small number\n",
            " |        of features) from \"Sparse Projection Oblique Random Forests\", Tomita et\n",
            " |        al., 2020. Default: \"AXIS_ALIGNED\".\n",
            " |    uplift_min_examples_in_treatment: For uplift models only. Minimum number of\n",
            " |      examples per treatment in a node. Default: 5.\n",
            " |    uplift_split_score: For uplift models only. Splitter score i.e. score\n",
            " |      optimized by the splitters. The scores are introduced in \"Decision trees\n",
            " |      for uplift modeling with single and multiple treatments\", Rzepakowski et\n",
            " |      al. Notation: `p` probability / average value of the positive outcome,\n",
            " |      `q` probability / average value in the control group.\n",
            " |      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)\n",
            " |      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2\n",
            " |      - `CHI_SQUARED` or `CS`: (p-q)^2/q\n",
            " |        Default: \"KULLBACK_LEIBLER\".\n",
            " |    winner_take_all: Control how classification trees vote. If true, each tree\n",
            " |      votes for one class. If false, each tree vote for a distribution of\n",
            " |      classes. winner_take_all_inference=false is often preferable. Default:\n",
            " |      True.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RandomForestModel\n",
            " |      tensorflow_decision_forests.keras.wrappers.RandomForestModel\n",
            " |      tensorflow_decision_forests.keras.core.CoreModel\n",
            " |      keras.engine.training.Model\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      keras.utils.version_utils.LayerVersionSelector\n",
            " |      keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods inherited from tensorflow_decision_forests.keras.wrappers.RandomForestModel:\n",
            " |  \n",
            " |  __init__ = wrapper(*args, **kargs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from tensorflow_decision_forests.keras.wrappers.RandomForestModel:\n",
            " |  \n",
            " |  capabilities() -> yggdrasil_decision_forests.learner.abstract_learner_pb2.LearnerCapabilities\n",
            " |      Lists the capabilities of the learning algorithm.\n",
            " |  \n",
            " |  predefined_hyperparameters() -> List[tensorflow_decision_forests.keras.core.HyperParameterTemplate]\n",
            " |      Returns a better than default set of hyper-parameters.\n",
            " |      \n",
            " |      They can be used directly with the `hyperparameter_template` argument of the\n",
            " |      model constructor.\n",
            " |      \n",
            " |      These hyper-parameters outperforms the default hyper-parameters (either\n",
            " |      generally or in specific scenarios). Like default hyper-parameters, existing\n",
            " |      pre-defined hyper-parameters cannot change.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow_decision_forests.keras.core.CoreModel:\n",
            " |  \n",
            " |  call(self, inputs, training=False)\n",
            " |      Inference of the model.\n",
            " |      \n",
            " |      This method is used for prediction and evaluation of a trained model.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensors.\n",
            " |        training: Is the model being trained. Always False.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Model predictions.\n",
            " |  \n",
            " |  call_get_leaves(self, inputs)\n",
            " |      Computes the index of the active leaf in each tree.\n",
            " |      \n",
            " |      The active leaf is the leave that that receive the example during inference.\n",
            " |      \n",
            " |      The returned value \"leaves[i,j]\" is the index of the active leave for the\n",
            " |      i-th example and the j-th tree. Leaves are indexed by depth first\n",
            " |      exploration with the negative child visited before the positive one\n",
            " |      (similarly as \"iterate_on_nodes()\" iteration). Leaf indices are also\n",
            " |      available with LeafNode.leaf_idx.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensors. Same signature as the model's \"call(inputs)\".\n",
            " |      \n",
            " |      Returns:\n",
            " |        Index of the active leaf for each tree in the model.\n",
            " |  \n",
            " |  collect_data_step(self, data, is_training_example)\n",
            " |      Collect examples e.g. training or validation.\n",
            " |  \n",
            " |  compile(self, metrics=None, weighted_metrics=None)\n",
            " |      Configure the model for training.\n",
            " |      \n",
            " |      Unlike for most Keras model, calling \"compile\" is optional before calling\n",
            " |      \"fit\".\n",
            " |      \n",
            " |      Args:\n",
            " |        metrics: List of metrics to be evaluated by the model during training and\n",
            " |          testing.\n",
            " |        weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |          `sample_weight` or `class_weight` during training and testing.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: Invalid arguments.\n",
            " |  \n",
            " |  evaluate(self, *args, **kwargs)\n",
            " |      Returns the loss value & metrics values for the model.\n",
            " |      \n",
            " |      See details on `keras.Model.evaluate`.\n",
            " |      \n",
            " |      Args:\n",
            " |        *args: Passed to `keras.Model.evaluate`.\n",
            " |        **kwargs: Passed to `keras.Model.evaluate`.  Scalar test loss (if the\n",
            " |          model has a single output and no metrics) or list of scalars (if the\n",
            " |          model has multiple outputs and/or metrics). See details in\n",
            " |          `keras.Model.evaluate`.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, callbacks=None, verbose: Union[int, NoneType] = None, **kwargs) -> keras.callbacks.History\n",
            " |      Trains the model.\n",
            " |      \n",
            " |      The following dataset formats are supported:\n",
            " |      \n",
            " |        1. \"x\" is a tf.data.Dataset containing a tuple \"(features, labels)\".\n",
            " |           \"features\" can be a dictionary a tensor, a list of tensors or a\n",
            " |           dictionary of tensors (recommended). \"labels\" is a tensor.\n",
            " |      \n",
            " |        2. \"x\" is a tensor, list of tensors or dictionary of tensors containing\n",
            " |           the input features. \"y\" is a tensor.\n",
            " |      \n",
            " |        3. \"x\" is a numpy-array, list of numpy-arrays or dictionary of\n",
            " |           numpy-arrays containing the input features. \"y\" is a numpy-array.\n",
            " |      \n",
            " |      Unlike classical neural networks, the learning algorithm requires to scan\n",
            " |      the training dataset exactly once. Therefore, the dataset should not be\n",
            " |      repeated. The algorithm also does not benefit from shuffling the dataset.\n",
            " |      \n",
            " |      Input features generally do not need to be normalized (numerical) or indexed\n",
            " |      (categorical features stored as string). Also, missing values are well\n",
            " |      supported (i.e. not need to replace missing values).\n",
            " |      \n",
            " |      Pandas Dataframe can be prepared with \"dataframe_to_tf_dataset\":\n",
            " |        dataset = pandas.Dataframe(...)\n",
            " |        model.fit(pd_dataframe_to_tf_dataset(dataset, label=\"my_label\"))\n",
            " |      \n",
            " |      Some of the learning algorithm will support distributed training with the\n",
            " |      ParameterServerStrategy e.g.:\n",
            " |      \n",
            " |        with tf.distribute.experimental.ParameterServerStrategy(...).scope():\n",
            " |          model = DistributedGradientBoostedTreesModel()\n",
            " |        model.fit(...)\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Training dataset (See details above for the supported formats).\n",
            " |        y: Label of the training dataset. Only used if \"x\" does not contains the\n",
            " |          labels.\n",
            " |        callbacks: Callbacks triggered during the training.\n",
            " |        verbose: Verbosity mode. 0 = silent, 1 = small details, 2 = full details.\n",
            " |        **kwargs: Arguments passed to the core keras model's fit.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `History` object. Its `History.history` attribute is not yet\n",
            " |        implemented for decision forests algorithms, and will return empty.\n",
            " |        All other fields are filled as usual for `Keras.Mode.fit()`.\n",
            " |  \n",
            " |  fit_on_dataset_path(self, train_path: str, label_key: str, weight_key: Union[str, NoneType] = None, ranking_key: Union[str, NoneType] = None, valid_path: Union[str, NoneType] = None, dataset_format: Union[str, NoneType] = 'csv', max_num_scanned_rows_to_accumulate_statistics: Union[int, NoneType] = 100000, try_resume_training: Union[bool, NoneType] = True, input_model_signature_fn: Union[Callable[[tensorflow_decision_forests.component.inspector.inspector.AbstractInspector], Any], NoneType] = <function build_default_input_model_signature at 0x7feb1f0f9cb0>)\n",
            " |      Trains the model on a dataset stored on disk.\n",
            " |      \n",
            " |      This solution is generally more efficient and easier that loading the\n",
            " |      dataset with a tf.Dataset both for local and distributed training.\n",
            " |      \n",
            " |      Usage example:\n",
            " |      \n",
            " |        # Local training\n",
            " |        model = model = keras.GradientBoostedTreesModel()\n",
            " |        model.fit_on_dataset_path(\n",
            " |          train_path=\"/path/to/dataset.csv\",\n",
            " |          label_key=\"label\",\n",
            " |          dataset_format=\"csv\")\n",
            " |        model.save(\"/model/path\")\n",
            " |      \n",
            " |        # Distributed training\n",
            " |        with tf.distribute.experimental.ParameterServerStrategy(...).scope():\n",
            " |          model = model = keras.DistributedGradientBoostedTreesModel()\n",
            " |        model.fit_on_dataset_path(\n",
            " |          train_path=\"/path/to/dataset@10\",\n",
            " |          label_key=\"label\",\n",
            " |          dataset_format=\"tfrecord+tfe\")\n",
            " |        model.save(\"/model/path\")\n",
            " |      \n",
            " |      Args:\n",
            " |         train_path: Path to the training dataset. Support comma separated files,\n",
            " |           shard and glob notation.\n",
            " |         label_key: Name of the label column.\n",
            " |         weight_key: Name of the weighing column.\n",
            " |         ranking_key: Name of the ranking column.\n",
            " |         valid_path: Path to the validation dataset. If not provided, or if the\n",
            " |           learning algorithm does not support/need a validation dataset,\n",
            " |           `valid_path` is ignored.\n",
            " |         dataset_format: Format of the dataset. Should be one of the registered\n",
            " |           dataset format (see\n",
            " |           https://github.com/google/yggdrasil-decision-forests/blob/main/documentation/user_manual.md#dataset-path-and-format\n",
            " |             for more details). The format \"csv\" always available but it is\n",
            " |             generally only suited for small datasets.\n",
            " |        max_num_scanned_rows_to_accumulate_statistics: Maximum number of examples\n",
            " |          to scan to determine the statistics of the features (i.e. the dataspec,\n",
            " |          e.g. mean value, dictionaries). (Currently) the \"first\" examples of the\n",
            " |          dataset are scanned (e.g. the first examples of the dataset is a single\n",
            " |          file). Therefore, it is important that the sampled dataset is relatively\n",
            " |          uniformly sampled, notably the scanned examples should contains all the\n",
            " |          possible categorical values (otherwise the not seen value will be\n",
            " |          treated as out-of-vocabulary). If set to None, the entire dataset is\n",
            " |          scanned. This parameter has no effect if the dataset is stored in a\n",
            " |          format that already contains those values.\n",
            " |        try_resume_training: If true, tries to resume training from the model\n",
            " |          checkpoint stored in the `temp_directory` directory. If `temp_directory`\n",
            " |          does not contain any model checkpoint, start the training from the\n",
            " |          start. Works in the following three situations: (1) The training was\n",
            " |            interrupted by the user (e.g. ctrl+c). (2) the training job was\n",
            " |            interrupted (e.g. rescheduling), ond (3) the hyper-parameter of the\n",
            " |            model were changed such that an initially completed training is now\n",
            " |            incomplete (e.g. increasing the number of trees).\n",
            " |        input_model_signature_fn: A lambda that returns the\n",
            " |          (Dense,Sparse,Ragged)TensorSpec (or structure of TensorSpec e.g.\n",
            " |          dictionary, list) corresponding to input signature of the model. If not\n",
            " |          specified, the input model signature is created by\n",
            " |          \"build_default_input_model_signature\". For example, specify\n",
            " |          \"input_model_signature_fn\" if an numerical input feature (which is\n",
            " |          consumed as DenseTensorSpec(float32) by default) will be feed\n",
            " |          differently (e.g. RaggedTensor(int64)).\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `History` object. Its `History.history` attribute is not yet\n",
            " |        implemented for decision forests algorithms, and will return empty.\n",
            " |        All other fields are filled as usual for `Keras.Mode.fit()`.\n",
            " |  \n",
            " |  make_inspector(self) -> tensorflow_decision_forests.component.inspector.inspector.AbstractInspector\n",
            " |      Creates an inspector to access the internal model structure.\n",
            " |      \n",
            " |      Usage example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inspector = model.make_inspector()\n",
            " |      print(inspector.num_trees())\n",
            " |      print(inspector.variable_importances())\n",
            " |      ```\n",
            " |      \n",
            " |      Returns:\n",
            " |        A model inspector.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Prediction of the model (!= evaluation).\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Predictions for evaluation.\n",
            " |  \n",
            " |  predict_get_leaves(self, x)\n",
            " |      Gets the index of the active leaf of each tree.\n",
            " |      \n",
            " |      The active leaf is the leave that that receive the example during inference.\n",
            " |      \n",
            " |      The returned value \"leaves[i,j]\" is the index of the active leave for the\n",
            " |      i-th example and the j-th tree. Leaves are indexed by depth first\n",
            " |      exploration with the negative child visited before the positive one\n",
            " |      (similarly as \"iterate_on_nodes()\" iteration). Leaf indices are also\n",
            " |      available with LeafNode.leaf_idx.\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Input samples as a tf.data.Dataset.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Index of the active leaf for each tree in the model.\n",
            " |  \n",
            " |  save(self, filepath: str, overwrite: Union[bool, NoneType] = True, **kwargs)\n",
            " |      Saves the model as a TensorFlow SavedModel.\n",
            " |      \n",
            " |      The exported SavedModel contains a standalone Yggdrasil Decision Forests\n",
            " |      model in the \"assets\" sub-directory. The Yggdrasil model can be used\n",
            " |      directly using the Yggdrasil API. However, this model does not contain the\n",
            " |      \"preprocessing\" layer (if any).\n",
            " |      \n",
            " |      Args:\n",
            " |        filepath: Path to the output model.\n",
            " |        overwrite: If true, override an already existing model. If false, raise an\n",
            " |          error if a model already exist.\n",
            " |        **kwargs: Arguments passed to the core keras model's save.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Shows information about the model.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      Collects training examples.\n",
            " |  \n",
            " |  valid_step(self, data)\n",
            " |      Collects validation examples.\n",
            " |  \n",
            " |  yggdrasil_model_path_tensor(self) -> Union[tensorflow.python.framework.ops.Tensor, NoneType]\n",
            " |      Gets the path to yggdrasil model, if available.\n",
            " |      \n",
            " |      The effective path can be obtained with:\n",
            " |      \n",
            " |      ```python\n",
            " |      yggdrasil_model_path_tensor().numpy().decode(\"utf-8\")\n",
            " |      ```\n",
            " |      \n",
            " |      Returns:\n",
            " |        Path to the Yggdrasil model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow_decision_forests.keras.core.CoreModel:\n",
            " |  \n",
            " |  learner_params\n",
            " |      Gets the dictionary of hyper-parameters passed in the model constructor.\n",
            " |      \n",
            " |      Changing this dictionary will impact the training.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  __copy__(self)\n",
            " |  \n",
            " |  __deepcopy__(self, memo)\n",
            " |  \n",
            " |  __reduce__(self)\n",
            " |      Helper for pickle.\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes,\n",
            " |         where shapes are tuples, integers, or `TensorShape` instances.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, `TensorShape`, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or keyword arg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None)\n",
            " |      Compute the total loss, validate it, and return it.\n",
            " |      \n",
            " |      Subclasses can optionally override this method to provide custom loss\n",
            " |      computation logic.\n",
            " |      \n",
            " |      Example:\n",
            " |      ```python\n",
            " |      class MyModel(tf.keras.Model):\n",
            " |      \n",
            " |        def __init__(self, *args, **kwargs):\n",
            " |          super(MyModel, self).__init__(*args, **kwargs)\n",
            " |          self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
            " |      \n",
            " |        def compute_loss(self, x, y, y_pred, sample_weight):\n",
            " |          loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y))\n",
            " |          loss += tf.add_n(self.losses)\n",
            " |          self.loss_tracker.update_state(loss)\n",
            " |          return loss\n",
            " |      \n",
            " |        def reset_metrics(self):\n",
            " |          self.loss_tracker.reset_states()\n",
            " |      \n",
            " |        @property\n",
            " |        def metrics(self):\n",
            " |          return [self.loss_tracker]\n",
            " |      \n",
            " |      tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))\n",
            " |      dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n",
            " |      \n",
            " |      inputs = tf.keras.layers.Input(shape=(10,), name='my_input')\n",
            " |      outputs = tf.keras.layers.Dense(10)(inputs)\n",
            " |      model = MyModel(inputs, outputs)\n",
            " |      model.add_loss(tf.reduce_sum(outputs))\n",
            " |      \n",
            " |      optimizer = tf.keras.optimizers.SGD()\n",
            " |      model.compile(optimizer, loss='mse', steps_per_execution=10)\n",
            " |      model.fit(dataset, epochs=2, steps_per_epoch=10)\n",
            " |      print('My custom loss: ', model.loss_tracker.result().numpy())\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Input data.\n",
            " |        y: Target data.\n",
            " |        y_pred: Predictions returned by the model (output of `model(x)`)\n",
            " |        sample_weight: Sample weights for weighting the loss function.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The total loss as a `tf.Tensor`, or `None` if no loss results (which is\n",
            " |        the case when called by `Model.test_step`).\n",
            " |  \n",
            " |  compute_metrics(self, x, y, y_pred, sample_weight)\n",
            " |      Update metric states and collect all metrics to be returned.\n",
            " |      \n",
            " |      Subclasses can optionally override this method to provide custom metric\n",
            " |      updating and collection logic.\n",
            " |      \n",
            " |      Example:\n",
            " |      ```python\n",
            " |      class MyModel(tf.keras.Sequential):\n",
            " |      \n",
            " |        def compute_metrics(self, x, y, y_pred, sample_weight):\n",
            " |      \n",
            " |          # This super call updates `self.compiled_metrics` and returns results\n",
            " |          # for all metrics listed in `self.metrics`.\n",
            " |          metric_results = super(MyModel, self).compute_metrics(\n",
            " |              x, y, y_pred, sample_weight)\n",
            " |      \n",
            " |          # Note that `self.custom_metric` is not listed in `self.metrics`.\n",
            " |          self.custom_metric.update_state(x, y, y_pred, sample_weight)\n",
            " |          metric_results['custom_metric_name'] = self.custom_metric.result()\n",
            " |          return metric_results\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Input data.\n",
            " |        y: Target data.\n",
            " |        y_pred: Predictions returned by the model (output of `model.call(x)`)\n",
            " |        sample_weight: Sample weights for weighting the loss function.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n",
            " |        values of the metrics listed in `self.metrics` are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
            " |      every time it is called. The callers should make a copy of the returned dict\n",
            " |      if they want to modify it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Args:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Args:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`). This can also be a path to a SavedModel\n",
            " |              saved from `model.save`.\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If `h5py` is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_train_function(self, force=False)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called. You can skip the cache and generate again the\n",
            " |      function with `force=True`.\n",
            " |      \n",
            " |      Args:\n",
            " |        force: Whether to regenerate the train function and skip the cached\n",
            " |          function if available.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for batch processing\n",
            " |      of large numbers of inputs. It is not intended for use inside of loops\n",
            " |      that iterate over your data and process small numbers of inputs at a time.\n",
            " |      \n",
            " |      For small numbers of inputs that fit in one batch,\n",
            " |      directly use `__call__()` for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behave differently during\n",
            " |      inference. You may pair the individual model call with a `tf.function`\n",
            " |      for additional performance inside your inner loop.\n",
            " |      If you need access to numpy array values instead of tensors after your\n",
            " |      model call, you can use `tensor.numpy()` to get the numpy array value of\n",
            " |      an eager tensor.\n",
            " |      \n",
            " |      Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Note: See [this FAQ entry](\n",
            " |      https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n",
            " |      for more details about the difference between `Model` methods `predict()`\n",
            " |      and `__call__()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict()` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in a `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
            " |                model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
            " |                multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Args:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save_spec(self, dynamic_batch=True)\n",
            " |      Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`.\n",
            " |      \n",
            " |      This value is automatically defined after calling the model for the first\n",
            " |      time. Afterwards, you can use it when exporting the model for serving:\n",
            " |      \n",
            " |      ```python\n",
            " |      model = tf.keras.Model(...)\n",
            " |      \n",
            " |      @tf.function\n",
            " |      def serve(*args, **kwargs):\n",
            " |        outputs = model(*args, **kwargs)\n",
            " |        # Apply postprocessing steps, or add additional outputs.\n",
            " |        ...\n",
            " |        return outputs\n",
            " |      \n",
            " |      # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is\n",
            " |      # an empty dict since functional models do not use keyword arguments.\n",
            " |      arg_specs, kwarg_specs = model.save_spec()\n",
            " |      \n",
            " |      model.save(path, signatures={\n",
            " |        'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)\n",
            " |      })\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        dynamic_batch: Whether to set the batch sizes of all the returned\n",
            " |          `tf.TensorSpec` to `None`. (Note that when defining functional or\n",
            " |          Sequential models with `tf.keras.Input([...], batch_size=X)`, the\n",
            " |          batch size will always be preserved). Defaults to `True`.\n",
            " |      Returns:\n",
            " |        If the model inputs are defined, returns a tuple `(args, kwargs)`. All\n",
            " |        elements in `args` and `kwargs` are `tf.TensorSpec`.\n",
            " |        If the model inputs are not defined, returns `None`.\n",
            " |        The model inputs are automatically set when calling the model,\n",
            " |        `model.fit`, `model.evaluate` or `model.predict`.\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the\n",
            " |      [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint)\n",
            " |      for details on the TensorFlow format.\n",
            " |      \n",
            " |      Args:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If `h5py` is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays (in case the\n",
            " |                model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors (in case the model has\n",
            " |                multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |                the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Args:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      Note: Since TF 2.6, this method is no longer supported and will raise a\n",
            " |      RuntimeError.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: announces that the method poses a security risk\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Args:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.training.Model:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile()`, `add_metric()` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Args:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |        - If the layer is not built, the method will call `build`.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(inputs))\n",
            " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Args:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      This method will cause the layer's state to be built, if that has not\n",
            " |      happened before. This requires that the layer will later be used with\n",
            " |      inputs that match the input shape provided here.\n",
            " |      \n",
            " |      Args:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  finalize_state(self)\n",
            " |      Finalizes the layers state after updating layer weights.\n",
            " |      \n",
            " |      This function can be subclassed in a layer and will be called after updating\n",
            " |      a layer weights. It can be overridden to finalize any additional layer state\n",
            " |      after a weight update.\n",
            " |      \n",
            " |      This function will be called after weights of a layer have been restored\n",
            " |      from a loaded model.\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first input node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first output node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function, by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
            " |      and the bias vector. These can be used to set the weights of another\n",
            " |      `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Args:\n",
            " |        weights: a list of NumPy arrays. The number\n",
            " |          of arrays and their shape must match\n",
            " |          number of the dimensions of the weights\n",
            " |          of the layer (i.e. it should match the\n",
            " |          output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If the provided weights list does not match the\n",
            " |          layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a subset of features- GradientBoostedTreesModel**\n",
        "\n",
        "In the previous examples all features were taken into the model.\n",
        " Now we want to specify input features"
      ],
      "metadata": {
        "id": "wazRK3tgO1WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature1 = tfdf.keras.FeatureUsage(name=\"bill_length_mm\")\n",
        "feature2 = tfdf.keras.FeatureUsage(name=\"island\")\n",
        "\n",
        "all_features = [feature1, feature2]\n",
        "\n",
        "#this model is trained using only 2 features to it will be worse\n",
        "model_2 = tfdf.keras.GradientBoostedTreesModel(\n",
        "    features=all_features, exclude_non_specified_features=True)\n",
        "\n",
        "model_2.compile(metrics=[\"accuracy\"])\n",
        "model_2.fit(x=train_ds, validation_data=test_ds)\n",
        "\n",
        "print(model_2.evaluate(test_ds, return_dict=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG_WPi6zOu_y",
        "outputId": "62249ecf-5183-476a-c9d6-0a4974c820ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmprv_qfvit as temporary training directory\n",
            "Starting reading the dataset\n",
            "\r1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.275599\n",
            "Training model\n",
            "Model trained in 0:00:00.106835\n",
            "Compiling model\n",
            "1/1 [==============================] - 1s 567ms/step - val_loss: 0.0000e+00 - val_accuracy: 0.9495\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0000e+00 - accuracy: 0.9495\n",
            "{'loss': 0.0, 'accuracy': 0.9494949579238892}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-DF attaches a semantics to each feature, it controls how the feature is used by the model:\n",
        "\n",
        "- Numerical- for quantities or counts with full ordering, e.g. age, the number of items \n",
        "\n",
        "- Categorical - type/class of in finite set of possible values without ordering\n",
        " \n",
        "- Categorical-Set - a set of categorical values, can be string or integer "
      ],
      "metadata": {
        "id": "AylIXHvoPhyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a model that treats the year as a categorical feature."
      ],
      "metadata": {
        "id": "BpwsERczQsmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_1 = tfdf.keras.FeatureUsage(name=\"year\", semantic=tfdf.keras.FeatureSemantic.CATEGORICAL)\n",
        "feature_2 = tfdf.keras.FeatureUsage(name=\"bill_length_mm\")\n",
        "feature_3 = tfdf.keras.FeatureUsage(name=\"sex\")\n",
        "all_features = [feature_1, feature_2, feature_3]\n",
        "\n",
        "model_3 = tfdf.keras.GradientBoostedTreesModel(features=all_features, exclude_non_specified_features=True)\n",
        "model_3.compile( metrics=[\"accuracy\"])\n",
        "\n",
        "with sys_pipes():\n",
        "  model_3.fit(x=train_ds, validation_data= test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoKb38BtPgSw",
        "outputId": "cc4c2053-50b8-4ebf-d344-ed376044b685"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpesv2ozk9 as temporary training directory\n",
            "Starting reading the dataset\n",
            "1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.363279\n",
            "Training model\n",
            "Model trained in 0:00:00.183195\n",
            "Compiling model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO kernel.cc:1153] Loading model from path\n",
            "[INFO abstract_model.cc:1063] Engine \"GradientBoostedTreesGeneric\" built\n",
            "[INFO kernel.cc:1001] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 1s 971ms/step - val_loss: 0.0000e+00 - val_accuracy: 0.7273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper-parameters**\n",
        "\n",
        "parameters that impact the quality of a final model"
      ],
      "metadata": {
        "id": "u5AcpZS5RWK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a classical but slightly more complex model \n",
        "model_6 = tfdf.keras.GradientBoostedTreesModel(\n",
        "    num_trees=500, growing_strategy=\"BEST_FIRST_GLOBAL\", max_depth=8)\n",
        "model_6.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7tQSGLORF19",
        "outputId": "6e92ad62-24fe-4b06-ddc5-26dc6fb4144e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpdyrlrw6d as temporary training directory\n",
            "Starting reading the dataset\n",
            "1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.329910\n",
            "Training model\n",
            "Model trained in 0:00:07.049605\n",
            "Compiling model\n",
            "1/1 [==============================] - 8s 8s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb16aeff50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A more complex but possibly more accurate model \n",
        "model_6 = tfdf.keras.GradientBoostedTreesModel(\n",
        "    num_trees=500, \n",
        "    growing_strategy=\"BEST_FIRST_GLOBAL\", \n",
        "    max_depth=8,\n",
        "    split_axis=\"SPARSE_OBLIQUE\",\n",
        "    categorical_algorithm=\"RANDOM\")\n",
        "model_6.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0jESBNSR4xe",
        "outputId": "a3adf395-21ff-4fbb-f28e-9f31b5bc6fbc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpbgwrk8w9 as temporary training directory\n",
            "Starting reading the dataset\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7feb16a0c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7feb16a0c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.327665\n",
            "Training model\n",
            "Model trained in 0:00:06.294199\n",
            "Compiling model\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7feb169e14d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7feb169e14d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb16a06b10>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#There are some templates of good hyper-parameters, e.g. banchamrk_rank1\n",
        "model_8 = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\")\n",
        "model_8.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5O2N1adTdeO",
        "outputId": "c5b051f4-00e4-4a5d-d2dc-2ebcbee15e63"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolve hyper-parameter template \"benchmark_rank1\" to \"benchmark_rank1@v1\" -> {'growing_strategy': 'BEST_FIRST_GLOBAL', 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}.\n",
            "Use /tmp/tmpn4f24o25 as temporary training directory\n",
            "Starting reading the dataset\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7feb16a8a5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7feb16a8a5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.133403\n",
            "Training model\n",
            "Model trained in 0:00:00.905804\n",
            "Compiling model\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7feb164feb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7feb164feb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb164f2990>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hyper-parameter templates for the Gradient Boosted Tree model\n",
        "print(tfdf.keras.GradientBoostedTreesModel.predefined_hyperparameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLRx-Z4RTx-p",
        "outputId": "d9739b4f-56e9-492b-cb2b-7d8967f81828"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HyperParameterTemplate(name='better_default', version=1, parameters={'growing_strategy': 'BEST_FIRST_GLOBAL'}, description='A configuration that is generally better than the default parameters without being more expensive.'), HyperParameterTemplate(name='benchmark_rank1', version=1, parameters={'growing_strategy': 'BEST_FIRST_GLOBAL', 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}, description='Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Preprocessing**\n",
        "\n",
        "We will pre-process the body_mass_g into body_mass-kg. "
      ],
      "metadata": {
        "id": "L5d0VnuOUJ9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "body_mass_g = tf.keras.layers.Input(shape=(1,), name=\"body_mass_g\")\n",
        "body_mass_kg = body_mass_g / 1000.0\n",
        "\n",
        "bill_length_mm = tf.keras.layers.Input(shape=(1,), name=\"bill_length_mm\")\n",
        "\n",
        "raw_inputs = {\"body_mass_g\": body_mass_g, \"bill_length_mm\": bill_length_mm}\n",
        "processed_inputs = {\"body_mass_kg\": body_mass_kg, \"bill_length_mm\": bill_length_mm}\n",
        "\n",
        "# \"preprocessor\" contains the preprocessing logic.\n",
        "preprocessor = tf.keras.Model(inputs=raw_inputs, outputs=processed_inputs)\n",
        "\n",
        "# \"model_4\" contains both the pre-processing logic and the decision forest.\n",
        "model_4 = tfdf.keras.RandomForestModel(preprocessing=preprocessor)\n",
        "model_4.fit(x=train_ds)\n",
        "\n",
        "model_4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "fXyxMjPcUHIx",
        "outputId": "3bbc6378-401d-4326-f783-a959b5dfaea6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp6hd9nqi0 as temporary training directory\n",
            "Starting reading the dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['island', 'bill_depth_mm', 'flipper_length_mm', 'sex', 'year'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.519562\n",
            "Training model\n",
            "Model trained in 0:00:00.106494\n",
            "Compiling model\n",
            "1/1 [==============================] - 1s 679ms/step\n",
            "Model: \"random_forest_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (Functional)          {'body_mass_kg': (None,   0         \n",
            "                             1),                                 \n",
            "                              'bill_length_mm': (None            \n",
            "                             , 1)}                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1\n",
            "Trainable params: 0\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n",
            "Type: \"RANDOM_FOREST\"\n",
            "Task: CLASSIFICATION\n",
            "Label: \"__LABEL\"\n",
            "\n",
            "Input Features (2):\n",
            "\tbill_length_mm\n",
            "\tbody_mass_kg\n",
            "\n",
            "No weights\n",
            "\n",
            "Variable Importance: MEAN_MIN_DEPTH:\n",
            "    1.        \"__LABEL\"  3.683800 ################\n",
            "    2.   \"body_mass_kg\"  1.148949 ####\n",
            "    3. \"bill_length_mm\"  0.022111 \n",
            "\n",
            "Variable Importance: NUM_AS_ROOT:\n",
            "    1. \"bill_length_mm\" 294.000000 ################\n",
            "    2.   \"body_mass_kg\"  6.000000 \n",
            "\n",
            "Variable Importance: NUM_NODES:\n",
            "    1. \"bill_length_mm\" 1315.000000 ################\n",
            "    2.   \"body_mass_kg\" 1241.000000 \n",
            "\n",
            "Variable Importance: SUM_SCORE:\n",
            "    1. \"bill_length_mm\" 44667.861599 ################\n",
            "    2.   \"body_mass_kg\" 26971.348486 \n",
            "\n",
            "\n",
            "\n",
            "Winner take all: true\n",
            "Out-of-bag evaluation: accuracy:0.926531 logloss:0.459622\n",
            "Number of trees: 300\n",
            "Total number of nodes: 5412\n",
            "\n",
            "Number of nodes by tree:\n",
            "Count: 300 Average: 18.04 StdDev: 2.60993\n",
            "Min: 13 Max: 27 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 13, 14) 16   5.33%   5.33% ##\n",
            "[ 14, 15)  0   0.00%   5.33%\n",
            "[ 15, 16) 56  18.67%  24.00% #######\n",
            "[ 16, 17)  0   0.00%  24.00%\n",
            "[ 17, 18) 73  24.33%  48.33% #########\n",
            "[ 18, 19)  0   0.00%  48.33%\n",
            "[ 19, 20) 85  28.33%  76.67% ##########\n",
            "[ 20, 21)  0   0.00%  76.67%\n",
            "[ 21, 22) 56  18.67%  95.33% #######\n",
            "[ 22, 23)  0   0.00%  95.33%\n",
            "[ 23, 24) 10   3.33%  98.67% #\n",
            "[ 24, 25)  0   0.00%  98.67%\n",
            "[ 25, 26)  3   1.00%  99.67%\n",
            "[ 26, 27)  0   0.00%  99.67%\n",
            "[ 27, 27]  1   0.33% 100.00%\n",
            "\n",
            "Depth by leafs:\n",
            "Count: 2856 Average: 3.71604 StdDev: 1.26091\n",
            "Min: 1 Max: 8 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 1, 2)  15   0.53%   0.53%\n",
            "[ 2, 3) 439  15.37%  15.90% #####\n",
            "[ 3, 4) 964  33.75%  49.65% ##########\n",
            "[ 4, 5) 713  24.96%  74.61% #######\n",
            "[ 5, 6) 437  15.30%  89.92% #####\n",
            "[ 6, 7) 231   8.09%  98.00% ##\n",
            "[ 7, 8)  51   1.79%  99.79% #\n",
            "[ 8, 8]   6   0.21% 100.00%\n",
            "\n",
            "Number of training obs by leaf:\n",
            "Count: 2856 Average: 25.7353 StdDev: 30.8239\n",
            "Min: 5 Max: 114 Ignored: 0\n",
            "----------------------------------------------\n",
            "[   5,  10) 1781  62.36%  62.36% ##########\n",
            "[  10,  16)  172   6.02%  68.38% #\n",
            "[  16,  21)    3   0.11%  68.49%\n",
            "[  21,  27)    7   0.25%  68.73%\n",
            "[  27,  32)   27   0.95%  69.68%\n",
            "[  32,  38)   89   3.12%  72.79%\n",
            "[  38,  43)   83   2.91%  75.70%\n",
            "[  43,  49)   75   2.63%  78.33%\n",
            "[  49,  54)   20   0.70%  79.03%\n",
            "[  54,  60)   23   0.81%  79.83%\n",
            "[  60,  65)   38   1.33%  81.16%\n",
            "[  65,  71)   70   2.45%  83.61%\n",
            "[  71,  76)  101   3.54%  87.15% #\n",
            "[  76,  82)   88   3.08%  90.23%\n",
            "[  82,  87)   69   2.42%  92.65%\n",
            "[  87,  93)   80   2.80%  95.45%\n",
            "[  93,  98)   64   2.24%  97.69%\n",
            "[  98, 104)   41   1.44%  99.12%\n",
            "[ 104, 109)   16   0.56%  99.68%\n",
            "[ 109, 114]    9   0.32% 100.00%\n",
            "\n",
            "Attribute in nodes:\n",
            "\t1315 : bill_length_mm [NUMERICAL]\n",
            "\t1241 : body_mass_kg [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 0:\n",
            "\t294 : bill_length_mm [NUMERICAL]\n",
            "\t6 : body_mass_kg [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 1:\n",
            "\t461 : body_mass_kg [NUMERICAL]\n",
            "\t424 : bill_length_mm [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 2:\n",
            "\t872 : body_mass_kg [NUMERICAL]\n",
            "\t744 : bill_length_mm [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 3:\n",
            "\t1080 : bill_length_mm [NUMERICAL]\n",
            "\t1034 : body_mass_kg [NUMERICAL]\n",
            "\n",
            "Attribute in nodes with depth <= 5:\n",
            "\t1294 : bill_length_mm [NUMERICAL]\n",
            "\t1232 : body_mass_kg [NUMERICAL]\n",
            "\n",
            "Condition type in nodes:\n",
            "\t2556 : HigherCondition\n",
            "Condition type in nodes with depth <= 0:\n",
            "\t300 : HigherCondition\n",
            "Condition type in nodes with depth <= 1:\n",
            "\t885 : HigherCondition\n",
            "Condition type in nodes with depth <= 2:\n",
            "\t1616 : HigherCondition\n",
            "Condition type in nodes with depth <= 3:\n",
            "\t2114 : HigherCondition\n",
            "Condition type in nodes with depth <= 5:\n",
            "\t2526 : HigherCondition\n",
            "Node format: NOT_SET\n",
            "\n",
            "Training OOB:\n",
            "\ttrees: 1, Out-of-bag evaluation: accuracy:0.913043 logloss:3.13423\n",
            "\ttrees: 11, Out-of-bag evaluation: accuracy:0.91358 logloss:1.42158\n",
            "\ttrees: 21, Out-of-bag evaluation: accuracy:0.930612 logloss:1.40662\n",
            "\ttrees: 31, Out-of-bag evaluation: accuracy:0.926531 logloss:0.856375\n",
            "\ttrees: 41, Out-of-bag evaluation: accuracy:0.918367 logloss:0.860458\n",
            "\ttrees: 51, Out-of-bag evaluation: accuracy:0.926531 logloss:0.728473\n",
            "\ttrees: 61, Out-of-bag evaluation: accuracy:0.926531 logloss:0.732807\n",
            "\ttrees: 71, Out-of-bag evaluation: accuracy:0.922449 logloss:0.726739\n",
            "\ttrees: 81, Out-of-bag evaluation: accuracy:0.922449 logloss:0.72784\n",
            "\ttrees: 91, Out-of-bag evaluation: accuracy:0.922449 logloss:0.726156\n",
            "\ttrees: 101, Out-of-bag evaluation: accuracy:0.926531 logloss:0.589465\n",
            "\ttrees: 111, Out-of-bag evaluation: accuracy:0.926531 logloss:0.591583\n",
            "\ttrees: 121, Out-of-bag evaluation: accuracy:0.926531 logloss:0.592226\n",
            "\ttrees: 131, Out-of-bag evaluation: accuracy:0.926531 logloss:0.59536\n",
            "\ttrees: 141, Out-of-bag evaluation: accuracy:0.926531 logloss:0.597183\n",
            "\ttrees: 151, Out-of-bag evaluation: accuracy:0.926531 logloss:0.595868\n",
            "\ttrees: 161, Out-of-bag evaluation: accuracy:0.926531 logloss:0.596623\n",
            "\ttrees: 171, Out-of-bag evaluation: accuracy:0.926531 logloss:0.59431\n",
            "\ttrees: 181, Out-of-bag evaluation: accuracy:0.930612 logloss:0.593691\n",
            "\ttrees: 191, Out-of-bag evaluation: accuracy:0.930612 logloss:0.595031\n",
            "\ttrees: 201, Out-of-bag evaluation: accuracy:0.922449 logloss:0.596607\n",
            "\ttrees: 211, Out-of-bag evaluation: accuracy:0.922449 logloss:0.598349\n",
            "\ttrees: 221, Out-of-bag evaluation: accuracy:0.922449 logloss:0.594611\n",
            "\ttrees: 231, Out-of-bag evaluation: accuracy:0.922449 logloss:0.593347\n",
            "\ttrees: 241, Out-of-bag evaluation: accuracy:0.918367 logloss:0.591857\n",
            "\ttrees: 251, Out-of-bag evaluation: accuracy:0.922449 logloss:0.591343\n",
            "\ttrees: 261, Out-of-bag evaluation: accuracy:0.922449 logloss:0.591848\n",
            "\ttrees: 271, Out-of-bag evaluation: accuracy:0.922449 logloss:0.462522\n",
            "\ttrees: 281, Out-of-bag evaluation: accuracy:0.926531 logloss:0.461682\n",
            "\ttrees: 291, Out-of-bag evaluation: accuracy:0.922449 logloss:0.460297\n",
            "\ttrees: 300, Out-of-bag evaluation: accuracy:0.926531 logloss:0.459622\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same logic but with TensorFlow Feature Columns"
      ],
      "metadata": {
        "id": "-KMb9rooUoj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g_to_kg(x):\n",
        "  return x / 1000\n",
        "\n",
        "feature_columns = [\n",
        "    tf.feature_column.numeric_column(\"body_mass_g\", normalizer_fn=g_to_kg),\n",
        "    tf.feature_column.numeric_column(\"bill_length_mm\"),\n",
        "]\n",
        "\n",
        "preprocessing = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "model_5 = tfdf.keras.RandomForestModel(preprocessing=preprocessing)\n",
        "model_5.compile(metrics=[\"accuracy\"])\n",
        "model_5.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1GJgH0oUkTe",
        "outputId": "b7f74385-7b97-4aa2-9ec0-123af223483f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmpu1acvlhr as temporary training directory\n",
            "Starting reading the dataset\n",
            "1/1 [==============================] - ETA: 0s\n",
            "Dataset read in 0:00:00.214843\n",
            "Training model\n",
            "Model trained in 0:00:00.083415\n",
            "Compiling model\n",
            "1/1 [==============================] - 0s 339ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb17147790>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wDylhOa1Ux5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}